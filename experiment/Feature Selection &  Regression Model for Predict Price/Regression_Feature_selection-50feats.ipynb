{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the copy based on copy1, to conduct PCA before regression.** <br>\n",
    "Due to large dimension and computing capacity, must reduce dimension before doing regression.<br>\n",
    "Last version, failed to run RFE for feature selection. RFE is better when choosing a few features from a relatively small feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:31:08.518914Z",
     "start_time": "2021-05-10T16:31:08.509938Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\lzowe\\OneDrive - The City College of New York\\CCNY_Course\\Applied_Machine_Learning_and_Data_Mining\\codes\\project-product-price-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:31:11.017229Z",
     "start_time": "2021-05-10T16:31:08.837063Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from final.feature_extraction.vectorization import text_vectorizaion\n",
    "from final.dimension_reduction.feature_reduction import dimension_reduction\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 15)\n",
    "plt.rcParams['figure.constrained_layout.use'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data processed using Tokenizing and tf-idf algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:31:18.357739Z",
     "start_time": "2021-05-10T16:31:11.051139Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/DSEI21000-S21/project-product-price-prediction/main/data/random_samples/stratified_sampling_clean_text_data_by_price_whigh_sz50000_1619835594.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:31:18.467463Z",
     "start_time": "2021-05-10T16:31:18.453459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 34)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:31:18.672871Z",
     "start_time": "2021-05-10T16:31:18.563164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>item_description_bef_word_count</th>\n",
       "      <th>item_description_bef_char_count</th>\n",
       "      <th>item_description_bef_avg_word_len</th>\n",
       "      <th>item_description_upper_word_count</th>\n",
       "      <th>item_description_upper_char_count</th>\n",
       "      <th>item_description_stopword_count</th>\n",
       "      <th>item_description_punctuation_count</th>\n",
       "      <th>item_description_number_count</th>\n",
       "      <th>item_description_after_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>item_name_upper_char_count</th>\n",
       "      <th>item_name_stopword_count</th>\n",
       "      <th>item_name_punctuation_count</th>\n",
       "      <th>item_name_number_count</th>\n",
       "      <th>item_name_after_word_count</th>\n",
       "      <th>item_name_after_char_count</th>\n",
       "      <th>item_name_after_avg_word_len</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>shipping</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.373508e+05</td>\n",
       "      <td>26.583180</td>\n",
       "      <td>150.390920</td>\n",
       "      <td>5.667798</td>\n",
       "      <td>1.876040</td>\n",
       "      <td>12.577260</td>\n",
       "      <td>7.67970</td>\n",
       "      <td>5.850200</td>\n",
       "      <td>0.499880</td>\n",
       "      <td>18.323300</td>\n",
       "      <td>...</td>\n",
       "      <td>4.794540</td>\n",
       "      <td>0.192960</td>\n",
       "      <td>0.414340</td>\n",
       "      <td>0.177680</td>\n",
       "      <td>4.236300</td>\n",
       "      <td>24.381740</td>\n",
       "      <td>5.845573</td>\n",
       "      <td>1.991820</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>108.41749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.251891e+05</td>\n",
       "      <td>28.205148</td>\n",
       "      <td>161.300927</td>\n",
       "      <td>0.782677</td>\n",
       "      <td>5.826971</td>\n",
       "      <td>28.114883</td>\n",
       "      <td>10.36772</td>\n",
       "      <td>8.336725</td>\n",
       "      <td>1.229061</td>\n",
       "      <td>18.691275</td>\n",
       "      <td>...</td>\n",
       "      <td>5.226287</td>\n",
       "      <td>0.463088</td>\n",
       "      <td>0.828128</td>\n",
       "      <td>0.422508</td>\n",
       "      <td>1.535705</td>\n",
       "      <td>8.740065</td>\n",
       "      <td>1.210724</td>\n",
       "      <td>0.896911</td>\n",
       "      <td>0.484564</td>\n",
       "      <td>198.75487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.726685e+05</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>5.210526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.353620e+05</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>5.642857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.102881e+06</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>6.020944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.482519e+06</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>761.000000</td>\n",
       "      <td>104.00000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2009.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           train_id  item_description_bef_word_count  \\\n",
       "count  5.000000e+04                     50000.000000   \n",
       "mean   7.373508e+05                        26.583180   \n",
       "std    4.251891e+05                        28.205148   \n",
       "min    1.900000e+01                         1.000000   \n",
       "25%    3.726685e+05                         8.000000   \n",
       "50%    7.353620e+05                        17.000000   \n",
       "75%    1.102881e+06                        34.000000   \n",
       "max    1.482519e+06                       206.000000   \n",
       "\n",
       "       item_description_bef_char_count  item_description_bef_avg_word_len  \\\n",
       "count                     50000.000000                       50000.000000   \n",
       "mean                        150.390920                           5.667798   \n",
       "std                         161.300927                           0.782677   \n",
       "min                           1.000000                           1.000000   \n",
       "25%                          46.000000                           5.210526   \n",
       "50%                          97.000000                           5.642857   \n",
       "75%                         191.000000                           6.020944   \n",
       "max                        1007.000000                          19.600000   \n",
       "\n",
       "       item_description_upper_word_count  item_description_upper_char_count  \\\n",
       "count                       50000.000000                       50000.000000   \n",
       "mean                            1.876040                          12.577260   \n",
       "std                             5.826971                          28.114883   \n",
       "min                             0.000000                           0.000000   \n",
       "25%                             0.000000                           2.000000   \n",
       "50%                             0.000000                           5.000000   \n",
       "75%                             1.000000                          12.000000   \n",
       "max                           178.000000                         761.000000   \n",
       "\n",
       "       item_description_stopword_count  item_description_punctuation_count  \\\n",
       "count                      50000.00000                        50000.000000   \n",
       "mean                           7.67970                            5.850200   \n",
       "std                           10.36772                            8.336725   \n",
       "min                            0.00000                            0.000000   \n",
       "25%                            1.00000                            1.000000   \n",
       "50%                            4.00000                            3.000000   \n",
       "75%                           10.00000                            8.000000   \n",
       "max                          104.00000                          308.000000   \n",
       "\n",
       "       item_description_number_count  item_description_after_word_count  ...  \\\n",
       "count                   50000.000000                       50000.000000  ...   \n",
       "mean                        0.499880                          18.323300  ...   \n",
       "std                         1.229061                          18.691275  ...   \n",
       "min                         0.000000                           1.000000  ...   \n",
       "25%                         0.000000                           7.000000  ...   \n",
       "50%                         0.000000                          12.000000  ...   \n",
       "75%                         1.000000                          23.000000  ...   \n",
       "max                        57.000000                         175.000000  ...   \n",
       "\n",
       "       item_name_upper_char_count  item_name_stopword_count  \\\n",
       "count                50000.000000              50000.000000   \n",
       "mean                     4.794540                  0.192960   \n",
       "std                      5.226287                  0.463088   \n",
       "min                      0.000000                  0.000000   \n",
       "25%                      2.000000                  0.000000   \n",
       "50%                      3.000000                  0.000000   \n",
       "75%                      6.000000                  0.000000   \n",
       "max                     37.000000                  6.000000   \n",
       "\n",
       "       item_name_punctuation_count  item_name_number_count  \\\n",
       "count                 50000.000000            50000.000000   \n",
       "mean                      0.414340                0.177680   \n",
       "std                       0.828128                0.422508   \n",
       "min                       0.000000                0.000000   \n",
       "25%                       0.000000                0.000000   \n",
       "50%                       0.000000                0.000000   \n",
       "75%                       1.000000                0.000000   \n",
       "max                      12.000000                9.000000   \n",
       "\n",
       "       item_name_after_word_count  item_name_after_char_count  \\\n",
       "count                50000.000000                50000.000000   \n",
       "mean                     4.236300                   24.381740   \n",
       "std                      1.535705                    8.740065   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      3.000000                   18.000000   \n",
       "50%                      4.000000                   25.000000   \n",
       "75%                      5.000000                   32.000000   \n",
       "max                     13.000000                   42.000000   \n",
       "\n",
       "       item_name_after_avg_word_len  item_condition_id      shipping  \\\n",
       "count                  50000.000000       50000.000000  50000.000000   \n",
       "mean                       5.845573           1.991820      0.376700   \n",
       "std                        1.210724           0.896911      0.484564   \n",
       "min                        1.000000           1.000000      0.000000   \n",
       "25%                        5.000000           1.000000      0.000000   \n",
       "50%                        5.750000           2.000000      0.000000   \n",
       "75%                        6.500000           3.000000      1.000000   \n",
       "max                       26.000000           5.000000      1.000000   \n",
       "\n",
       "             price  \n",
       "count  50000.00000  \n",
       "mean     108.41749  \n",
       "std      198.75487  \n",
       "min        3.00000  \n",
       "25%       20.00000  \n",
       "50%       50.00000  \n",
       "75%       90.00000  \n",
       "max     2009.00000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:31:18.799530Z",
     "start_time": "2021-05-10T16:31:18.754652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 34 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   train_id                             50000 non-null  int64  \n",
      " 1   clean_item_description               50000 non-null  object \n",
      " 2   item_description_bef_word_count      50000 non-null  float64\n",
      " 3   item_description_bef_char_count      50000 non-null  float64\n",
      " 4   item_description_bef_avg_word_len    50000 non-null  float64\n",
      " 5   item_description_upper_word_count    50000 non-null  float64\n",
      " 6   item_description_upper_char_count    50000 non-null  float64\n",
      " 7   item_description_stopword_count      50000 non-null  float64\n",
      " 8   item_description_punctuation_count   50000 non-null  float64\n",
      " 9   item_description_number_count        50000 non-null  float64\n",
      " 10  item_description_after_word_count    50000 non-null  float64\n",
      " 11  item_description_after_char_count    50000 non-null  float64\n",
      " 12  item_description_after_avg_word_len  50000 non-null  float64\n",
      " 13  clean_item_name                      50000 non-null  object \n",
      " 14  item_name_bef_word_count             50000 non-null  float64\n",
      " 15  item_name_bef_char_count             50000 non-null  float64\n",
      " 16  item_name_bef_avg_word_len           50000 non-null  float64\n",
      " 17  item_name_upper_word_count           50000 non-null  float64\n",
      " 18  item_name_upper_char_count           50000 non-null  float64\n",
      " 19  item_name_stopword_count             50000 non-null  float64\n",
      " 20  item_name_punctuation_count          50000 non-null  float64\n",
      " 21  item_name_number_count               50000 non-null  float64\n",
      " 22  item_name_after_word_count           50000 non-null  float64\n",
      " 23  item_name_after_char_count           50000 non-null  float64\n",
      " 24  item_name_after_avg_word_len         50000 non-null  float64\n",
      " 25  item_condition_id                    50000 non-null  int64  \n",
      " 26  category_name                        50000 non-null  object \n",
      " 27  brand_name                           50000 non-null  object \n",
      " 28  shipping                             50000 non-null  int64  \n",
      " 29  price                                50000 non-null  float64\n",
      " 30  c1                                   50000 non-null  object \n",
      " 31  c2                                   50000 non-null  object \n",
      " 32  c3                                   50000 non-null  object \n",
      " 33  price_bin                            50000 non-null  object \n",
      "dtypes: float64(23), int64(3), object(8)\n",
      "memory usage: 13.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:31:18.910235Z",
     "start_time": "2021-05-10T16:31:18.895292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42039,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.clean_item_name.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:31:19.035940Z",
     "start_time": "2021-05-10T16:31:19.005981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>clean_item_description</th>\n",
       "      <th>item_description_bef_word_count</th>\n",
       "      <th>item_description_bef_char_count</th>\n",
       "      <th>item_description_bef_avg_word_len</th>\n",
       "      <th>item_description_upper_word_count</th>\n",
       "      <th>item_description_upper_char_count</th>\n",
       "      <th>item_description_stopword_count</th>\n",
       "      <th>item_description_punctuation_count</th>\n",
       "      <th>item_description_number_count</th>\n",
       "      <th>...</th>\n",
       "      <th>item_name_after_avg_word_len</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>shipping</th>\n",
       "      <th>price</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>price_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>806824</td>\n",
       "      <td>new tags</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Athletic Apparel/Shirts &amp; Tops</td>\n",
       "      <td>Nike</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>women</td>\n",
       "      <td>athletic apparel</td>\n",
       "      <td>shirts &amp; tops</td>\n",
       "      <td>(10, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>772820</td>\n",
       "      <td>nastasya every hills lipstick fashion</td>\n",
       "      <td>6.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Beauty/Makeup/Lips</td>\n",
       "      <td>Anastasia Beverly Hills</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>beauty</td>\n",
       "      <td>makeup</td>\n",
       "      <td>lips</td>\n",
       "      <td>(20, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1423115</td>\n",
       "      <td>brand new tags taken bag pictures</td>\n",
       "      <td>11.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>4.909091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jeans/Leggings</td>\n",
       "      <td>LuLaRoe</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>women</td>\n",
       "      <td>jeans</td>\n",
       "      <td>leggings</td>\n",
       "      <td>(50, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>405853</td>\n",
       "      <td>bought calves bit large frowned good condition...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>5.371429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Shoes/Boots</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>women</td>\n",
       "      <td>shoes</td>\n",
       "      <td>boots</td>\n",
       "      <td>(80, 90]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1172086</td>\n",
       "      <td>brand new box size 7youth859womens</td>\n",
       "      <td>6.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Shoes/Athletic</td>\n",
       "      <td>Nike</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>women</td>\n",
       "      <td>shoes</td>\n",
       "      <td>athletic</td>\n",
       "      <td>(50, 60]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                             clean_item_description  \\\n",
       "0    806824                                           new tags   \n",
       "1    772820              nastasya every hills lipstick fashion   \n",
       "2   1423115                  brand new tags taken bag pictures   \n",
       "3    405853  bought calves bit large frowned good condition...   \n",
       "4   1172086                 brand new box size 7youth859womens   \n",
       "\n",
       "   item_description_bef_word_count  item_description_bef_char_count  \\\n",
       "0                              3.0                             13.0   \n",
       "1                              6.0                             42.0   \n",
       "2                             11.0                             54.0   \n",
       "3                             35.0                            188.0   \n",
       "4                              6.0                             40.0   \n",
       "\n",
       "   item_description_bef_avg_word_len  item_description_upper_word_count  \\\n",
       "0                           4.333333                                0.0   \n",
       "1                           7.000000                                0.0   \n",
       "2                           4.909091                                0.0   \n",
       "3                           5.371429                                1.0   \n",
       "4                           6.666667                                0.0   \n",
       "\n",
       "   item_description_upper_char_count  item_description_stopword_count  \\\n",
       "0                                1.0                              1.0   \n",
       "1                                4.0                              1.0   \n",
       "2                                1.0                              5.0   \n",
       "3                                4.0                             17.0   \n",
       "4                                3.0                              1.0   \n",
       "\n",
       "   item_description_punctuation_count  item_description_number_count  ...  \\\n",
       "0                                 0.0                            0.0  ...   \n",
       "1                                 0.0                            0.0  ...   \n",
       "2                                 0.0                            0.0  ...   \n",
       "3                                 7.0                            1.0  ...   \n",
       "4                                 3.0                            0.0  ...   \n",
       "\n",
       "   item_name_after_avg_word_len  item_condition_id  \\\n",
       "0                      5.250000                  1   \n",
       "1                     10.000000                  1   \n",
       "2                      6.166667                  1   \n",
       "3                      5.333333                  3   \n",
       "4                      4.000000                  1   \n",
       "\n",
       "                          category_name               brand_name  shipping  \\\n",
       "0  Women/Athletic Apparel/Shirts & Tops                     Nike         1   \n",
       "1                    Beauty/Makeup/Lips  Anastasia Beverly Hills         0   \n",
       "2                  Women/Jeans/Leggings                  LuLaRoe         0   \n",
       "3                     Women/Shoes/Boots                   Hunter         0   \n",
       "4                  Women/Shoes/Athletic                     Nike         0   \n",
       "\n",
       "   price      c1                c2             c3  price_bin  \n",
       "0   15.0   women  athletic apparel  shirts & tops   (10, 15]  \n",
       "1   22.0  beauty            makeup           lips   (20, 25]  \n",
       "2   54.0   women             jeans       leggings   (50, 60]  \n",
       "3   84.0   women             shoes          boots   (80, 90]  \n",
       "4   56.0   women             shoes       athletic   (50, 60]  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature extraction and dimension selection\n",
    " \n",
    " For Item-discription feature: <br>\n",
    " using Jin's function to, firstly, do feature-extraction, increasing up to 14230 new few features <br>\n",
    " secondly, do dimenstion-reduction <br>\n",
    " finally, left 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:31:19.147600Z",
     "start_time": "2021-05-10T16:31:19.132640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corners bottom great shape lips smells markings inside cleanthere small water mark indicated third photo comes dusting'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.clean_item_description[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:33:22.092202Z",
     "start_time": "2021-05-10T16:33:20.122442Z"
    }
   },
   "outputs": [],
   "source": [
    "description_feature,  description_feature_name = text_vectorizaion(df, text_col = \"clean_item_description\", \n",
    "                                                                   tfidf = True, min_df=10, max_features=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:33:22.204190Z",
     "start_time": "2021-05-10T16:33:22.192189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 14230)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:21.329612Z",
     "start_time": "2021-05-10T16:34:16.621210Z"
    }
   },
   "outputs": [],
   "source": [
    "data = dimension_reduction(description_feature.toarray(), method = 'SVD', n_comp = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:24.209240Z",
     "start_time": "2021-05-10T16:35:24.184344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:25.691702Z",
     "start_time": "2021-05-10T16:35:25.543130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.316579</td>\n",
       "      <td>0.226464</td>\n",
       "      <td>-0.093895</td>\n",
       "      <td>-0.145724</td>\n",
       "      <td>-0.160603</td>\n",
       "      <td>-0.003326</td>\n",
       "      <td>-0.018080</td>\n",
       "      <td>0.090899</td>\n",
       "      <td>0.191937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>-0.009510</td>\n",
       "      <td>-0.002965</td>\n",
       "      <td>-0.012123</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>-0.004534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>-0.003074</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>-0.002301</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010199</td>\n",
       "      <td>0.006671</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>-0.005570</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>-0.022127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.282559</td>\n",
       "      <td>0.198734</td>\n",
       "      <td>-0.108194</td>\n",
       "      <td>-0.022315</td>\n",
       "      <td>-0.049011</td>\n",
       "      <td>-0.093210</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>-0.023776</td>\n",
       "      <td>0.027208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017481</td>\n",
       "      <td>-0.020886</td>\n",
       "      <td>0.010136</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>-0.004427</td>\n",
       "      <td>0.027268</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>-0.017329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.087350</td>\n",
       "      <td>-0.138583</td>\n",
       "      <td>-0.028886</td>\n",
       "      <td>-0.007240</td>\n",
       "      <td>-0.041191</td>\n",
       "      <td>-0.030505</td>\n",
       "      <td>0.157934</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>-0.003727</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>-0.033910</td>\n",
       "      <td>0.021533</td>\n",
       "      <td>-0.046227</td>\n",
       "      <td>-0.022477</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>-0.003304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.407447</td>\n",
       "      <td>0.230953</td>\n",
       "      <td>-0.090665</td>\n",
       "      <td>-0.028014</td>\n",
       "      <td>-0.120190</td>\n",
       "      <td>-0.079098</td>\n",
       "      <td>0.052897</td>\n",
       "      <td>-0.152960</td>\n",
       "      <td>-0.178794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>-0.013488</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.026397</td>\n",
       "      <td>0.021090</td>\n",
       "      <td>-0.001629</td>\n",
       "      <td>0.017143</td>\n",
       "      <td>0.015061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000323  0.316579  0.226464 -0.093895 -0.145724 -0.160603 -0.003326   \n",
       "1  0.000036  0.006363 -0.001485 -0.000308 -0.003074  0.008710  0.000230   \n",
       "2  0.000307  0.282559  0.198734 -0.108194 -0.022315 -0.049011 -0.093210   \n",
       "3  0.000196  0.087350 -0.138583 -0.028886 -0.007240 -0.041191 -0.030505   \n",
       "4  0.000409  0.407447  0.230953 -0.090665 -0.028014 -0.120190 -0.079098   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0 -0.018080  0.090899  0.191937  ... -0.000162  0.006184  0.001493  0.002215   \n",
       "1 -0.001265 -0.002301  0.001763  ...  0.010199  0.006671  0.000297 -0.005570   \n",
       "2  0.013352 -0.023776  0.027208  ... -0.017481 -0.020886  0.010136  0.022030   \n",
       "3  0.157934  0.007519  0.001815  ...  0.000969 -0.003727  0.006926 -0.033910   \n",
       "4  0.052897 -0.152960 -0.178794  ...  0.005267 -0.013488  0.000658 -0.005519   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.004637 -0.009510 -0.002965 -0.012123  0.000925 -0.004534  \n",
       "1  0.000096  0.005439 -0.000294  0.003562  0.002618 -0.022127  \n",
       "2 -0.004427  0.027268  0.028090  0.001704  0.008308 -0.017329  \n",
       "3  0.021533 -0.046227 -0.022477  0.008177  0.009686 -0.003304  \n",
       "4  0.001502  0.026397  0.021090 -0.001629  0.017143  0.015061  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cid = pd.DataFrame(data.copy()) #df for cleaned item description transforming to new features\n",
    "df_cid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating new features df_cid and previous df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:27.040302Z",
     "start_time": "2021-05-10T16:35:26.965177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>item_description_bef_word_count</th>\n",
       "      <th>item_description_bef_char_count</th>\n",
       "      <th>item_description_bef_avg_word_len</th>\n",
       "      <th>item_description_upper_word_count</th>\n",
       "      <th>item_description_upper_char_count</th>\n",
       "      <th>item_description_stopword_count</th>\n",
       "      <th>item_description_punctuation_count</th>\n",
       "      <th>item_description_number_count</th>\n",
       "      <th>item_description_after_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>item_name_after_avg_word_len</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>shipping</th>\n",
       "      <th>price</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>price_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>806824</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Athletic Apparel/Shirts &amp; Tops</td>\n",
       "      <td>Nike</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>women</td>\n",
       "      <td>athletic apparel</td>\n",
       "      <td>shirts &amp; tops</td>\n",
       "      <td>(10, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>772820</td>\n",
       "      <td>6.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Beauty/Makeup/Lips</td>\n",
       "      <td>Anastasia Beverly Hills</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>beauty</td>\n",
       "      <td>makeup</td>\n",
       "      <td>lips</td>\n",
       "      <td>(20, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1423115</td>\n",
       "      <td>11.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>4.909091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jeans/Leggings</td>\n",
       "      <td>LuLaRoe</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>women</td>\n",
       "      <td>jeans</td>\n",
       "      <td>leggings</td>\n",
       "      <td>(50, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>405853</td>\n",
       "      <td>35.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>5.371429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Shoes/Boots</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>women</td>\n",
       "      <td>shoes</td>\n",
       "      <td>boots</td>\n",
       "      <td>(80, 90]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1172086</td>\n",
       "      <td>6.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Shoes/Athletic</td>\n",
       "      <td>Nike</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>women</td>\n",
       "      <td>shoes</td>\n",
       "      <td>athletic</td>\n",
       "      <td>(50, 60]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id  item_description_bef_word_count  item_description_bef_char_count  \\\n",
       "0    806824                              3.0                             13.0   \n",
       "1    772820                              6.0                             42.0   \n",
       "2   1423115                             11.0                             54.0   \n",
       "3    405853                             35.0                            188.0   \n",
       "4   1172086                              6.0                             40.0   \n",
       "\n",
       "   item_description_bef_avg_word_len  item_description_upper_word_count  \\\n",
       "0                           4.333333                                0.0   \n",
       "1                           7.000000                                0.0   \n",
       "2                           4.909091                                0.0   \n",
       "3                           5.371429                                1.0   \n",
       "4                           6.666667                                0.0   \n",
       "\n",
       "   item_description_upper_char_count  item_description_stopword_count  \\\n",
       "0                                1.0                              1.0   \n",
       "1                                4.0                              1.0   \n",
       "2                                1.0                              5.0   \n",
       "3                                4.0                             17.0   \n",
       "4                                3.0                              1.0   \n",
       "\n",
       "   item_description_punctuation_count  item_description_number_count  \\\n",
       "0                                 0.0                            0.0   \n",
       "1                                 0.0                            0.0   \n",
       "2                                 0.0                            0.0   \n",
       "3                                 7.0                            1.0   \n",
       "4                                 3.0                            0.0   \n",
       "\n",
       "   item_description_after_word_count  ...  item_name_after_avg_word_len  \\\n",
       "0                                2.0  ...                      5.250000   \n",
       "1                                5.0  ...                     10.000000   \n",
       "2                                6.0  ...                      6.166667   \n",
       "3                               17.0  ...                      5.333333   \n",
       "4                                5.0  ...                      4.000000   \n",
       "\n",
       "   item_condition_id                         category_name  \\\n",
       "0                  1  Women/Athletic Apparel/Shirts & Tops   \n",
       "1                  1                    Beauty/Makeup/Lips   \n",
       "2                  1                  Women/Jeans/Leggings   \n",
       "3                  3                     Women/Shoes/Boots   \n",
       "4                  1                  Women/Shoes/Athletic   \n",
       "\n",
       "                brand_name  shipping  price      c1                c2  \\\n",
       "0                     Nike         1   15.0   women  athletic apparel   \n",
       "1  Anastasia Beverly Hills         0   22.0  beauty            makeup   \n",
       "2                  LuLaRoe         0   54.0   women             jeans   \n",
       "3                   Hunter         0   84.0   women             shoes   \n",
       "4                     Nike         0   56.0   women             shoes   \n",
       "\n",
       "              c3  price_bin  \n",
       "0  shirts & tops   (10, 15]  \n",
       "1           lips   (20, 25]  \n",
       "2       leggings   (50, 60]  \n",
       "3          boots   (80, 90]  \n",
       "4       athletic   (50, 60]  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df.copy()\n",
    "df1.drop(\"clean_item_description\", inplace=True,axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:27.295609Z",
     "start_time": "2021-05-10T16:35:27.249747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 33 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   train_id                             50000 non-null  int64  \n",
      " 1   item_description_bef_word_count      50000 non-null  float64\n",
      " 2   item_description_bef_char_count      50000 non-null  float64\n",
      " 3   item_description_bef_avg_word_len    50000 non-null  float64\n",
      " 4   item_description_upper_word_count    50000 non-null  float64\n",
      " 5   item_description_upper_char_count    50000 non-null  float64\n",
      " 6   item_description_stopword_count      50000 non-null  float64\n",
      " 7   item_description_punctuation_count   50000 non-null  float64\n",
      " 8   item_description_number_count        50000 non-null  float64\n",
      " 9   item_description_after_word_count    50000 non-null  float64\n",
      " 10  item_description_after_char_count    50000 non-null  float64\n",
      " 11  item_description_after_avg_word_len  50000 non-null  float64\n",
      " 12  clean_item_name                      50000 non-null  object \n",
      " 13  item_name_bef_word_count             50000 non-null  float64\n",
      " 14  item_name_bef_char_count             50000 non-null  float64\n",
      " 15  item_name_bef_avg_word_len           50000 non-null  float64\n",
      " 16  item_name_upper_word_count           50000 non-null  float64\n",
      " 17  item_name_upper_char_count           50000 non-null  float64\n",
      " 18  item_name_stopword_count             50000 non-null  float64\n",
      " 19  item_name_punctuation_count          50000 non-null  float64\n",
      " 20  item_name_number_count               50000 non-null  float64\n",
      " 21  item_name_after_word_count           50000 non-null  float64\n",
      " 22  item_name_after_char_count           50000 non-null  float64\n",
      " 23  item_name_after_avg_word_len         50000 non-null  float64\n",
      " 24  item_condition_id                    50000 non-null  int64  \n",
      " 25  category_name                        50000 non-null  object \n",
      " 26  brand_name                           50000 non-null  object \n",
      " 27  shipping                             50000 non-null  int64  \n",
      " 28  price                                50000 non-null  float64\n",
      " 29  c1                                   50000 non-null  object \n",
      " 30  c2                                   50000 non-null  object \n",
      " 31  c3                                   50000 non-null  object \n",
      " 32  price_bin                            50000 non-null  object \n",
      "dtypes: float64(23), int64(3), object(7)\n",
      "memory usage: 12.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:27.516390Z",
     "start_time": "2021-05-10T16:35:27.509411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 33), (50000, 100))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape,df_cid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:28.585734Z",
     "start_time": "2021-05-10T16:35:28.518918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 133)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat=pd.concat([df1,df_cid],axis=1)\n",
    "df_concat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one-hot encoding for category and nominal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete price bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:29.213133Z",
     "start_time": "2021-05-10T16:35:29.198132Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_catNnom = ['category_name','brand_name', 'c1', 'c2', 'c3'] # columns of category and nominal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:31.652356Z",
     "start_time": "2021-05-10T16:35:29.468683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3028)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from final.feature_encoding.one_hot_encoding import one_hot_encode_feature\n",
    "df_encode = df_concat\n",
    "for col in cols_catNnom:\n",
    "    df_encode, col_encode = one_hot_encode_feature(df_encode, encode_column=col,drop_first=False)\n",
    "df_encode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:32.657927Z",
     "start_time": "2021-05-10T16:35:32.630006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Columns: 3028 entries, train_id to c3_yoga & pilates\n",
      "dtypes: float64(123), int64(3), object(2), uint8(2900)\n",
      "memory usage: 187.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_encode.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:32.937220Z",
     "start_time": "2021-05-10T16:35:32.907811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>item_description_bef_word_count</th>\n",
       "      <th>item_description_bef_char_count</th>\n",
       "      <th>item_description_bef_avg_word_len</th>\n",
       "      <th>item_description_upper_word_count</th>\n",
       "      <th>item_description_upper_char_count</th>\n",
       "      <th>item_description_stopword_count</th>\n",
       "      <th>item_description_punctuation_count</th>\n",
       "      <th>item_description_number_count</th>\n",
       "      <th>item_description_after_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>c3_window treatments</th>\n",
       "      <th>c3_wine, beer &amp; beverage coolers</th>\n",
       "      <th>c3_wipes &amp; holders</th>\n",
       "      <th>c3_women</th>\n",
       "      <th>c3_women's golf clubs</th>\n",
       "      <th>c3_wool</th>\n",
       "      <th>c3_work &amp; safety</th>\n",
       "      <th>c3_wrap</th>\n",
       "      <th>c3_writing</th>\n",
       "      <th>c3_yoga &amp; pilates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>806824</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>772820</td>\n",
       "      <td>6.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1423115</td>\n",
       "      <td>11.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>4.909091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>405853</td>\n",
       "      <td>35.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>5.371429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1172086</td>\n",
       "      <td>6.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3028 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id  item_description_bef_word_count  item_description_bef_char_count  \\\n",
       "0    806824                              3.0                             13.0   \n",
       "1    772820                              6.0                             42.0   \n",
       "2   1423115                             11.0                             54.0   \n",
       "3    405853                             35.0                            188.0   \n",
       "4   1172086                              6.0                             40.0   \n",
       "\n",
       "   item_description_bef_avg_word_len  item_description_upper_word_count  \\\n",
       "0                           4.333333                                0.0   \n",
       "1                           7.000000                                0.0   \n",
       "2                           4.909091                                0.0   \n",
       "3                           5.371429                                1.0   \n",
       "4                           6.666667                                0.0   \n",
       "\n",
       "   item_description_upper_char_count  item_description_stopword_count  \\\n",
       "0                                1.0                              1.0   \n",
       "1                                4.0                              1.0   \n",
       "2                                1.0                              5.0   \n",
       "3                                4.0                             17.0   \n",
       "4                                3.0                              1.0   \n",
       "\n",
       "   item_description_punctuation_count  item_description_number_count  \\\n",
       "0                                 0.0                            0.0   \n",
       "1                                 0.0                            0.0   \n",
       "2                                 0.0                            0.0   \n",
       "3                                 7.0                            1.0   \n",
       "4                                 3.0                            0.0   \n",
       "\n",
       "   item_description_after_word_count  ...  c3_window treatments  \\\n",
       "0                                2.0  ...                     0   \n",
       "1                                5.0  ...                     0   \n",
       "2                                6.0  ...                     0   \n",
       "3                               17.0  ...                     0   \n",
       "4                                5.0  ...                     0   \n",
       "\n",
       "   c3_wine, beer & beverage coolers c3_wipes & holders  c3_women  \\\n",
       "0                                 0                  0         0   \n",
       "1                                 0                  0         0   \n",
       "2                                 0                  0         0   \n",
       "3                                 0                  0         0   \n",
       "4                                 0                  0         0   \n",
       "\n",
       "   c3_women's golf clubs  c3_wool  c3_work & safety  c3_wrap  c3_writing  \\\n",
       "0                      0        0                 0        0           0   \n",
       "1                      0        0                 0        0           0   \n",
       "2                      0        0                 0        0           0   \n",
       "3                      0        0                 0        0           0   \n",
       "4                      0        0                 0        0           0   \n",
       "\n",
       "   c3_yoga & pilates  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "\n",
       "[5 rows x 3028 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:33.220597Z",
     "start_time": "2021-05-10T16:35:33.206682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>1609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        price\n",
       "0        15.0\n",
       "1        22.0\n",
       "2        54.0\n",
       "3        84.0\n",
       "4        56.0\n",
       "...       ...\n",
       "49995  1609.0\n",
       "49996   205.0\n",
       "49997    36.0\n",
       "49998    20.0\n",
       "49999    72.0\n",
       "\n",
       "[50000 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encode[[\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:33.680514Z",
     "start_time": "2021-05-10T16:35:33.475029Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare input X, y\n",
    "X, y = df_encode.copy().drop([\"clean_item_name\",\"train_id\",\"price\",\"price_bin\"],axis=1), df_encode[[\"price\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:34.090592Z",
     "start_time": "2021-05-10T16:35:34.062596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Columns: 3024 entries, item_description_bef_word_count to c3_yoga & pilates\n",
      "dtypes: float64(122), int64(2), uint8(2900)\n",
      "memory usage: 185.6 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:34.797544Z",
     "start_time": "2021-05-10T16:35:34.408844Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct dimension reduction on the dateframe processed by one-hot encoding\n",
    "skip this step in the first place. <br>\n",
    "let us see how the regression result looks like. and then determine if the dimension reduction on whole dataset needed, or if use other techniques to avoid overfitting(e.g. adding penalty, using RobustScaler, conducting cross validation).<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* log 1: actually can't skip this step, the dimension is too large to run, Exception was raised: MemoryError: Unable to allocate 11.8 GiB for an array with shape (45079, 35000) and data type float64.\n",
    "Therefore, try conduct PCA again before regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* log 2: even can't not run PCA due to the large number of features. Try drop clean_item_name instead of encoding it, as it is has very less duplicated items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* log 3: Since log2 works, skip this step( e.i. 1.5) for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* log 4: features are too much later, have to reduce dimension at this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:38.502059Z",
     "start_time": "2021-05-10T16:35:35.426625Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:43.005417Z",
     "start_time": "2021-05-10T16:35:38.970097Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00290452, 0.00227656, 0.00213737, 0.00197211, 0.0018782 ,\n",
       "       0.00186   , 0.00180486, 0.00177907, 0.00174713, 0.00167492,\n",
       "       0.00161504, 0.00159662, 0.00154249, 0.00153705, 0.00145978,\n",
       "       0.00145388, 0.00144623, 0.00143838, 0.00143009, 0.00141804,\n",
       "       0.00141467, 0.00141096, 0.00140848, 0.00140426, 0.00139535,\n",
       "       0.0013908 , 0.00138366, 0.00137919, 0.00137138, 0.00136821,\n",
       "       0.00135522, 0.00135229, 0.00134384, 0.00133781, 0.0013262 ,\n",
       "       0.00130941, 0.00129653, 0.00128607, 0.0012781 , 0.00127189,\n",
       "       0.00126516, 0.00125693, 0.00125102, 0.00124276, 0.00123412,\n",
       "       0.00121429, 0.00120497, 0.00120168, 0.0011877 , 0.00116963])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50) # reduce features to 50\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:35:43.523396Z",
     "start_time": "2021-05-10T16:35:43.508461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 50), (15000, 50))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca.shape, X_test_pca.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model Selection\n",
    "Proposal:\n",
    "1. try different regression techniques, and select the best one;\n",
    "      - DT regressor\n",
    "      * Ensemble method\n",
    "      - Adding polynomial items to multiple linear regression? \n",
    "      - Tensorflow.Keras\n",
    "2. focus on the selected one and tuning modelling.(Guessing propably would use Ensemble learning or Keras in the end)<br>\n",
    "**Based on the our previous experiment so far, our results have both large bias and variance.** Probably need to use more powerful algorithm to improve the bias, and then considering solve overfitting issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:36:17.397741Z",
     "start_time": "2021-05-10T16:36:17.382778Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score,r2_score \n",
    "def evaluate_dt_regressor(X_train, X_test, y_train, y_test,n_iter,max_depth):\n",
    "    # the maximum depth of the tree. If None, then nodes are expanded until all leaves are pure\n",
    "    MSE_test=[]\n",
    "    EVS_test=[]\n",
    "    R2_test=[]\n",
    "    MSE_train=[]\n",
    "    EVS_train=[]\n",
    "    R2_train=[]\n",
    "    for i in range(n_iter):\n",
    "        rg = tree.DecisionTreeRegressor(max_depth=max_depth)\n",
    "        rg.fit(X_train, y_train)\n",
    "        y_train_pred = rg.predict(X_train)\n",
    "        y_pred = rg.predict(X_test)\n",
    "        MSE_train.append(mean_squared_error(y_train, y_train_pred))\n",
    "        EVS_train.append(explained_variance_score(y_train, y_train_pred))\n",
    "        R2_train.append(r2_score(y_train, y_train_pred))\n",
    "        \n",
    "        MSE_test.append(mean_squared_error(y_test, y_pred))\n",
    "        EVS_test.append(explained_variance_score(y_test, y_pred))\n",
    "        R2_test.append(r2_score(y_test, y_pred))\n",
    "    print(\"Train score:\",\"MSE: {}\".format(sum(MSE_train)/n_iter),\"EVS: {}\".format(sum(EVS_train)/n_iter),\"R2: {}\".format(sum(R2_train)/n_iter),)\n",
    "    print(\"Test score:\",\"MSE: {}\".format(sum(MSE_test)/n_iter),\"EVS: {}\".format(sum(EVS_test)/n_iter),\"R2: {}\".format(sum(R2_test)/n_iter),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:36:22.155915Z",
     "start_time": "2021-05-10T16:36:18.591458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: MSE: 1.4285714285714285e-05 EVS: 0.9999999996453222 R2: 0.9999999996453222\n",
      "Test score: MSE: 25563.02895 EVS: 0.32179118366443094 R2: 0.3217732698049083\n"
     ]
    }
   ],
   "source": [
    "evaluate_dt_regressor(X_train_pca, X_test_pca, y_train, y_test,1,max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:37:04.040927Z",
     "start_time": "2021-05-10T16:37:03.443344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: MSE: 28488.014051521317 EVS: 0.2927152881095668 R2: 0.2927152881095667\n",
      "Test score: MSE: 25593.412415670635 EVS: 0.32100058359066974 R2: 0.32096714942636884\n"
     ]
    }
   ],
   "source": [
    "evaluate_dt_regressor(X_train_pca, X_test_pca,y_train, y_test,1,max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:37:05.449182Z",
     "start_time": "2021-05-10T16:37:04.485305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: MSE: 23668.830832601077 EVS: 0.4123633130079254 R2: 0.4123633130079254\n",
      "Test score: MSE: 22268.274667473517 EVS: 0.40919334227164983 R2: 0.4091882012750778\n"
     ]
    }
   ],
   "source": [
    "evaluate_dt_regressor(X_train_pca, X_test_pca,  y_train, y_test,1,max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:37:02.999872Z",
     "start_time": "2021-05-10T16:36:26.964815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: MSE: 1.4285714285714289e-05 EVS: 0.9999999996453222 R2: 0.9999999996453222\n",
      "Test score: MSE: 25039.368356666666 EVS: 0.33575569278283396 R2: 0.3356667959845784\n"
     ]
    }
   ],
   "source": [
    "evaluate_dt_regressor(X_train_pca, X_test_pca,  y_train, y_test,10,max_depth=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:37:05.984896Z",
     "start_time": "2021-05-10T16:37:05.907769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 20.09742699,   3.33146439, -12.39210691,   0.20636868,\n",
       "         -7.80892106,  10.46098518,  -0.38913397,   7.1040594 ,\n",
       "        -22.88255888,  -6.51569881,   6.38323525,  -6.82233759,\n",
       "          0.50093439,  -2.98310321,  -5.80164048,  -0.41172873,\n",
       "          3.76489282,   0.81125286,  -3.11429064,  -3.28169822,\n",
       "         -5.37211941,  -0.27019337,  -2.79088729, -10.07583201,\n",
       "          6.37907991,   8.31669717,   4.69155748,   1.06908021,\n",
       "          5.11176258,  -3.84294021,   8.41289658,  -6.72101608,\n",
       "          7.22826676,  -2.66738023,   6.60358127,  -0.8122357 ,\n",
       "         -4.21020264,   4.44214887,  -3.88482086,  -3.62205375,\n",
       "          8.68074605,  -4.18221312,  -3.36449821,   0.14635779,\n",
       "         -2.44257515,  -0.75759656,   0.23913932,   3.70954428,\n",
       "         -2.14824853,  -1.30205249]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train_pca, y_train)\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why linear regression is so slow. after 1 and harf hour, not completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:37:06.517631Z",
     "start_time": "2021-05-10T16:37:06.504665Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, explained_variance_score,r2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:37:35.968547Z",
     "start_time": "2021-05-10T16:37:35.900729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'MSE': 28900.583072493755, 'EVS': 0.28247225184155333, 'R2': 0.28247225184155333}\n",
      "test: {'MSE': 26294.160732734363, 'EVS': 0.3024252728527732, 'R2': 0.3023752118002979}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train_pca, y_train)\n",
    "y_train_pred = reg.predict(X_train_pca)\n",
    "y_pred = reg.predict(X_test_pca)\n",
    "\n",
    "print(\"train:\",\n",
    "{\n",
    "    \"MSE\": mean_squared_error(y_train, y_train_pred), \n",
    "    \"EVS\": explained_variance_score(y_train, y_train_pred), \n",
    "    \"R2\": r2_score(y_train, y_train_pred)\n",
    "})\n",
    "\n",
    "print(\"test:\",\n",
    "{\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred), \n",
    "    \"EVS\": explained_variance_score(y_test, y_pred), \n",
    "    \"R2\": r2_score(y_test, y_pred)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:37:44.787224Z",
     "start_time": "2021-05-10T16:37:44.735403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'MSE': 28900.58307259148, 'EVS': 0.28247225183912716, 'R2': 0.28247225183912716}\n",
      "test: {'MSE': 26294.160725965943, 'EVS': 0.30242527305072064, 'R2': 0.3023752119798745}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5) # tuning alpha does not help\n",
    "reg.fit(X_train_pca, y_train)\n",
    "y_train_pred = reg.predict(X_train_pca)\n",
    "y_pred = reg.predict(X_test_pca)\n",
    "\n",
    "print(\"train:\",\n",
    "{\n",
    "    \"MSE\": mean_squared_error(y_train, y_train_pred), \n",
    "    \"EVS\": explained_variance_score(y_train, y_train_pred), \n",
    "    \"R2\": r2_score(y_train, y_train_pred)\n",
    "})\n",
    "\n",
    "print(\"test:\",\n",
    "{\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred), \n",
    "    \"EVS\": explained_variance_score(y_test, y_pred), \n",
    "    \"R2\": r2_score(y_test, y_pred)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:37:50.851684Z",
     "start_time": "2021-05-10T16:37:50.786854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'MSE': 28908.592080525163, 'EVS': 0.2822734086042614, 'R2': 0.2822734086042614}\n",
      "test: {'MSE': 26301.443289222272, 'EVS': 0.3022302678099197, 'R2': 0.3021819943031102}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.8) # tuning alpha does not help\n",
    "reg.fit(X_train_pca, y_train)\n",
    "y_train_pred = reg.predict(X_train_pca)\n",
    "y_pred = reg.predict(X_test_pca)\n",
    "\n",
    "print(\"train:\",\n",
    "{\n",
    "    \"MSE\": mean_squared_error(y_train, y_train_pred), \n",
    "    \"EVS\": explained_variance_score(y_train, y_train_pred), \n",
    "    \"R2\": r2_score(y_train, y_train_pred)\n",
    "})\n",
    "\n",
    "print(\"test:\",\n",
    "{\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred), \n",
    "    \"EVS\": explained_variance_score(y_test, y_pred), \n",
    "    \"R2\": r2_score(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (Bagging) for classification, also for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:37:55.005201Z",
     "start_time": "2021-05-10T16:37:54.942433Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def evaluate_rf_regressor(X_train, X_test, y_train, y_test,n_iter,max_depth,random_state):\n",
    "    MSE_test=[]\n",
    "    EVS_test=[]\n",
    "    R2_test=[]\n",
    "    MSE_train=[]\n",
    "    EVS_train=[]\n",
    "    R2_train=[]\n",
    "    for i in range(n_iter):\n",
    "        rg = RandomForestRegressor(max_depth=max_depth, random_state=random_state)\n",
    "        rg.fit(X_train, y_train)\n",
    "        y_train_pred = rg.predict(X_train)\n",
    "        y_pred = rg.predict(X_test)\n",
    "        MSE_train.append(mean_squared_error(y_train, y_train_pred))\n",
    "        EVS_train.append(explained_variance_score(y_train, y_train_pred))\n",
    "        R2_train.append(r2_score(y_train, y_train_pred))\n",
    "        \n",
    "        MSE_test.append(mean_squared_error(y_test, y_pred))\n",
    "        EVS_test.append(explained_variance_score(y_test, y_pred))\n",
    "        R2_test.append(r2_score(y_test, y_pred))\n",
    "    print(\"Train score:\",\"MSE: {}\".format(sum(MSE_train)/n_iter),\"EVS: {}\".format(sum(EVS_train)/n_iter),\"R2: {}\".format(sum(R2_train)/n_iter),)\n",
    "    print(\"Test score:\",\"MSE: {}\".format(sum(MSE_test)/n_iter),\"EVS: {}\".format(sum(EVS_test)/n_iter),\"R2: {}\".format(sum(R2_test)/n_iter),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf_regressor turns out very very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:41:23.752670Z",
     "start_time": "2021-05-10T16:37:57.828911Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-ffee7f25d399>:11: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rg.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: MSE: 995.4855787854625 EVS: 0.9758292430529518 R2: 0.9752846327052146\n",
      "Test score: MSE: 11136.67665048186 EVS: 0.7045604742079421 R2: 0.7045267286333712\n"
     ]
    }
   ],
   "source": [
    "evaluate_rf_regressor(X_train_pca, X_test_pca, y_train, y_test,1,max_depth=None,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T00:15:53.531190Z",
     "start_time": "2021-05-03T23:33:57.194172Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-558030b727bd>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rg.fit(X_train_pca, y_train)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MSE_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-558030b727bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_pca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_pca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mMSE_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mEVS_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplained_variance_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mR2_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MSE_train' is not defined"
     ]
    }
   ],
   "source": [
    "rg = RandomForestRegressor(max_depth=None, random_state=42)\n",
    "rg.fit(X_train_pca, y_train)\n",
    "y_train_pred = rg.predict(X_train_pca)\n",
    "y_pred = rg.predict(X_test_pca)\n",
    "MSE_train.append(mean_squared_error(y_train, y_train_pred))\n",
    "EVS_train.append(explained_variance_score(y_train, y_train_pred))\n",
    "R2_train.append(r2_score(y_train, y_train_pred))\n",
    "\n",
    "MSE_test.append(mean_squared_error(y_test, y_pred))\n",
    "EVS_test.append(explained_variance_score(y_test, y_pred))\n",
    "R2_test.append(r2_score(y_test, y_pred))\n",
    "print(\"Train score:\",\"MSE: {}\".format(sum(MSE_train)/n_iter),\"EVS: {}\".format(sum(EVS_train)/n_iter),\"R2: {}\".format(sum(R2_train)/n_iter),)\n",
    "print(\"Test score:\",\"MSE: {}\".format(sum(MSE_test)/n_iter),\"EVS: {}\".format(sum(EVS_test)/n_iter),\"R2: {}\".format(sum(R2_test)/n_iter),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T13:09:48.005739Z",
     "start_time": "2021-05-04T13:09:47.465304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = rg.feature_importances_\n",
    "importances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T13:09:49.846589Z",
     "start_time": "2021-05-04T13:09:49.835306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3024)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T13:14:07.663612Z",
     "start_time": "2021-05-04T13:14:07.573550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) 7                              0.105877\n",
      " 2) 24                             0.063736\n",
      " 3) 33                             0.055135\n",
      " 4) 288                            0.033101\n",
      " 5) 1                              0.030501\n",
      " 6) 10                             0.025732\n",
      " 7) 316                            0.017765\n",
      " 8) 89                             0.014740\n",
      " 9) 48                             0.012854\n",
      "10) 469                            0.012297\n",
      "11) 11                             0.010544\n",
      "12) 262                            0.009389\n",
      "13) 2                              0.008283\n",
      "14) 0                              0.007948\n",
      "15) 14                             0.007850\n",
      "16) 491                            0.007349\n",
      "17) 57                             0.006804\n",
      "18) 6                              0.006769\n",
      "19) 34                             0.006605\n",
      "20) 46                             0.006498\n",
      "21) 261                            0.005871\n",
      "22) 476                            0.005158\n",
      "23) 483                            0.004931\n",
      "24) 192                            0.004860\n",
      "25) 252                            0.004593\n",
      "26) 8                              0.004519\n",
      "27) 285                            0.004433\n",
      "28) 273                            0.004364\n",
      "29) 216                            0.004305\n",
      "30) 467                            0.004116\n",
      "31) 116                            0.003915\n",
      "32) 27                             0.003776\n",
      "33) 323                            0.003739\n",
      "34) 175                            0.003534\n",
      "35) 242                            0.003235\n",
      "36) 299                            0.003226\n",
      "37) 399                            0.003218\n",
      "38) 32                             0.003109\n",
      "39) 470                            0.002954\n",
      "40) 25                             0.002944\n",
      "41) 70                             0.002937\n",
      "42) 167                            0.002923\n",
      "43) 455                            0.002843\n",
      "44) 5                              0.002751\n",
      "45) 235                            0.002579\n",
      "46) 477                            0.002554\n",
      "47) 3                              0.002538\n",
      "48) 20                             0.002510\n",
      "49) 39                             0.002465\n",
      "50) 208                            0.002452\n",
      "51) 493                            0.002441\n",
      "52) 269                            0.002374\n",
      "53) 80                             0.002358\n",
      "54) 170                            0.002353\n",
      "55) 321                            0.002326\n",
      "56) 120                            0.002315\n",
      "57) 174                            0.002315\n",
      "58) 484                            0.002294\n",
      "59) 318                            0.002258\n",
      "60) 472                            0.002208\n",
      "61) 26                             0.002193\n",
      "62) 125                            0.002192\n",
      "63) 495                            0.002172\n",
      "64) 274                            0.002160\n",
      "65) 103                            0.002083\n",
      "66) 38                             0.002052\n",
      "67) 222                            0.002014\n",
      "68) 437                            0.001982\n",
      "69) 129                            0.001977\n",
      "70) 15                             0.001932\n",
      "71) 489                            0.001922\n",
      "72) 180                            0.001899\n",
      "73) 390                            0.001878\n",
      "74) 259                            0.001860\n",
      "75) 209                            0.001827\n",
      "76) 91                             0.001807\n",
      "77) 99                             0.001794\n",
      "78) 84                             0.001762\n",
      "79) 479                            0.001747\n",
      "80) 492                            0.001742\n",
      "81) 376                            0.001733\n",
      "82) 12                             0.001718\n",
      "83) 402                            0.001680\n",
      "84) 189                            0.001679\n",
      "85) 355                            0.001668\n",
      "86) 218                            0.001644\n",
      "87) 104                            0.001614\n",
      "88) 71                             0.001593\n",
      "89) 113                            0.001578\n",
      "90) 450                            0.001569\n",
      "91) 409                            0.001556\n",
      "92) 223                            0.001526\n",
      "93) 72                             0.001523\n",
      "94) 298                            0.001514\n",
      "95) 217                            0.001507\n",
      "96) 327                            0.001496\n",
      "97) 154                            0.001495\n",
      "98) 244                            0.001493\n",
      "99) 40                             0.001477\n",
      "100) 162                            0.001472\n",
      "101) 443                            0.001471\n",
      "102) 254                            0.001451\n",
      "103) 258                            0.001448\n",
      "104) 58                             0.001448\n",
      "105) 419                            0.001445\n",
      "106) 311                            0.001441\n",
      "107) 90                             0.001437\n",
      "108) 148                            0.001434\n",
      "109) 291                            0.001429\n",
      "110) 434                            0.001424\n",
      "111) 105                            0.001421\n",
      "112) 28                             0.001421\n",
      "113) 9                              0.001420\n",
      "114) 485                            0.001408\n",
      "115) 52                             0.001407\n",
      "116) 315                            0.001406\n",
      "117) 94                             0.001392\n",
      "118) 13                             0.001392\n",
      "119) 23                             0.001391\n",
      "120) 176                            0.001376\n",
      "121) 418                            0.001368\n",
      "122) 386                            0.001367\n",
      "123) 303                            0.001366\n",
      "124) 268                            0.001365\n",
      "125) 30                             0.001357\n",
      "126) 119                            0.001347\n",
      "127) 356                            0.001347\n",
      "128) 87                             0.001345\n",
      "129) 49                             0.001341\n",
      "130) 73                             0.001339\n",
      "131) 276                            0.001336\n",
      "132) 481                            0.001335\n",
      "133) 42                             0.001318\n",
      "134) 461                            0.001309\n",
      "135) 230                            0.001309\n",
      "136) 100                            0.001309\n",
      "137) 111                            0.001304\n",
      "138) 494                            0.001299\n",
      "139) 382                            0.001298\n",
      "140) 295                            0.001288\n",
      "141) 92                             0.001279\n",
      "142) 83                             0.001278\n",
      "143) 385                            0.001275\n",
      "144) 248                            0.001273\n",
      "145) 465                            0.001262\n",
      "146) 499                            0.001251\n",
      "147) 131                            0.001248\n",
      "148) 183                            0.001246\n",
      "149) 279                            0.001242\n",
      "150) 50                             0.001236\n",
      "151) 246                            0.001232\n",
      "152) 79                             0.001230\n",
      "153) 220                            0.001230\n",
      "154) 411                            0.001228\n",
      "155) 159                            0.001227\n",
      "156) 96                             0.001226\n",
      "157) 497                            0.001223\n",
      "158) 294                            0.001212\n",
      "159) 236                            0.001210\n",
      "160) 102                            0.001204\n",
      "161) 460                            0.001184\n",
      "162) 77                             0.001182\n",
      "163) 18                             0.001176\n",
      "164) 41                             0.001174\n",
      "165) 47                             0.001172\n",
      "166) 452                            0.001158\n",
      "167) 212                            0.001147\n",
      "168) 283                            0.001138\n",
      "169) 496                            0.001135\n",
      "170) 108                            0.001127\n",
      "171) 107                            0.001126\n",
      "172) 146                            0.001125\n",
      "173) 381                            0.001123\n",
      "174) 449                            0.001115\n",
      "175) 413                            0.001109\n",
      "176) 51                             0.001106\n",
      "177) 158                            0.001103\n",
      "178) 98                             0.001100\n",
      "179) 426                            0.001099\n",
      "180) 31                             0.001098\n",
      "181) 63                             0.001096\n",
      "182) 150                            0.001091\n",
      "183) 439                            0.001089\n",
      "184) 21                             0.001088\n",
      "185) 344                            0.001086\n",
      "186) 482                            0.001083\n",
      "187) 458                            0.001081\n",
      "188) 78                             0.001080\n",
      "189) 332                            0.001070\n",
      "190) 16                             0.001064\n",
      "191) 238                            0.001063\n",
      "192) 416                            0.001059\n",
      "193) 322                            0.001058\n",
      "194) 290                            0.001055\n",
      "195) 320                            0.001054\n",
      "196) 287                            0.001052\n",
      "197) 374                            0.001044\n",
      "198) 166                            0.001040\n",
      "199) 474                            0.001031\n",
      "200) 66                             0.001031\n",
      "201) 369                            0.001027\n",
      "202) 312                            0.001026\n",
      "203) 463                            0.001023\n",
      "204) 251                            0.001019\n",
      "205) 114                            0.001019\n",
      "206) 112                            0.001017\n",
      "207) 4                              0.001013\n",
      "208) 420                            0.001012\n",
      "209) 35                             0.001010\n",
      "210) 117                            0.001004\n",
      "211) 227                            0.001004\n",
      "212) 115                            0.001001\n",
      "213) 490                            0.001000\n",
      "214) 86                             0.000999\n",
      "215) 234                            0.000995\n",
      "216) 328                            0.000995\n",
      "217) 68                             0.000992\n",
      "218) 186                            0.000990\n",
      "219) 53                             0.000989\n",
      "220) 142                            0.000986\n",
      "221) 178                            0.000986\n",
      "222) 448                            0.000985\n",
      "223) 237                            0.000982\n",
      "224) 301                            0.000982\n",
      "225) 256                            0.000979\n",
      "226) 267                            0.000969\n",
      "227) 309                            0.000968\n",
      "228) 128                            0.000967\n",
      "229) 67                             0.000964\n",
      "230) 134                            0.000962\n",
      "231) 480                            0.000959\n",
      "232) 181                            0.000957\n",
      "233) 36                             0.000957\n",
      "234) 135                            0.000954\n",
      "235) 253                            0.000953\n",
      "236) 410                            0.000951\n",
      "237) 471                            0.000945\n",
      "238) 106                            0.000945\n",
      "239) 165                            0.000942\n",
      "240) 200                            0.000942\n",
      "241) 468                            0.000942\n",
      "242) 171                            0.000941\n",
      "243) 398                            0.000941\n",
      "244) 286                            0.000940\n",
      "245) 475                            0.000940\n",
      "246) 144                            0.000939\n",
      "247) 199                            0.000938\n",
      "248) 407                            0.000936\n",
      "249) 93                             0.000934\n",
      "250) 394                            0.000934\n",
      "251) 447                            0.000930\n",
      "252) 488                            0.000929\n",
      "253) 423                            0.000928\n",
      "254) 61                             0.000924\n",
      "255) 330                            0.000923\n",
      "256) 215                            0.000921\n",
      "257) 155                            0.000920\n",
      "258) 55                             0.000913\n",
      "259) 457                            0.000909\n",
      "260) 352                            0.000909\n",
      "261) 179                            0.000906\n",
      "262) 388                            0.000905\n",
      "263) 335                            0.000904\n",
      "264) 319                            0.000899\n",
      "265) 195                            0.000898\n",
      "266) 459                            0.000897\n",
      "267) 76                             0.000894\n",
      "268) 44                             0.000891\n",
      "269) 375                            0.000890\n",
      "270) 438                            0.000890\n",
      "271) 453                            0.000887\n",
      "272) 313                            0.000886\n",
      "273) 22                             0.000885\n",
      "274) 271                            0.000883\n",
      "275) 266                            0.000882\n",
      "276) 451                            0.000881\n",
      "277) 60                             0.000881\n",
      "278) 349                            0.000881\n",
      "279) 264                            0.000880\n",
      "280) 121                            0.000877\n",
      "281) 370                            0.000876\n",
      "282) 153                            0.000876\n",
      "283) 314                            0.000868\n",
      "284) 280                            0.000867\n",
      "285) 401                            0.000867\n",
      "286) 29                             0.000866\n",
      "287) 184                            0.000860\n",
      "288) 387                            0.000860\n",
      "289) 126                            0.000859\n",
      "290) 19                             0.000853\n",
      "291) 211                            0.000852\n",
      "292) 110                            0.000852\n",
      "293) 431                            0.000851\n",
      "294) 182                            0.000847\n",
      "295) 289                            0.000843\n",
      "296) 446                            0.000841\n",
      "297) 62                             0.000837\n",
      "298) 337                            0.000835\n",
      "299) 462                            0.000835\n",
      "300) 417                            0.000833\n",
      "301) 231                            0.000831\n",
      "302) 270                            0.000827\n",
      "303) 168                            0.000826\n",
      "304) 338                            0.000826\n",
      "305) 172                            0.000825\n",
      "306) 308                            0.000823\n",
      "307) 132                            0.000821\n",
      "308) 265                            0.000818\n",
      "309) 421                            0.000817\n",
      "310) 45                             0.000817\n",
      "311) 487                            0.000815\n",
      "312) 198                            0.000809\n",
      "313) 436                            0.000808\n",
      "314) 228                            0.000807\n",
      "315) 341                            0.000806\n",
      "316) 161                            0.000805\n",
      "317) 428                            0.000802\n",
      "318) 293                            0.000802\n",
      "319) 151                            0.000801\n",
      "320) 361                            0.000799\n",
      "321) 359                            0.000798\n",
      "322) 464                            0.000794\n",
      "323) 353                            0.000791\n",
      "324) 367                            0.000791\n",
      "325) 284                            0.000790\n",
      "326) 122                            0.000789\n",
      "327) 37                             0.000788\n",
      "328) 194                            0.000787\n",
      "329) 240                            0.000783\n",
      "330) 354                            0.000778\n",
      "331) 221                            0.000776\n",
      "332) 430                            0.000776\n",
      "333) 101                            0.000776\n",
      "334) 203                            0.000775\n",
      "335) 404                            0.000772\n",
      "336) 249                            0.000772\n",
      "337) 81                             0.000769\n",
      "338) 56                             0.000765\n",
      "339) 396                            0.000765\n",
      "340) 444                            0.000763\n",
      "341) 331                            0.000760\n",
      "342) 445                            0.000760\n",
      "343) 325                            0.000757\n",
      "344) 306                            0.000756\n",
      "345) 435                            0.000755\n",
      "346) 362                            0.000754\n",
      "347) 219                            0.000753\n",
      "348) 351                            0.000753\n",
      "349) 124                            0.000750\n",
      "350) 422                            0.000748\n",
      "351) 17                             0.000748\n",
      "352) 147                            0.000746\n",
      "353) 282                            0.000746\n",
      "354) 160                            0.000746\n",
      "355) 202                            0.000745\n",
      "356) 272                            0.000744\n",
      "357) 406                            0.000740\n",
      "358) 43                             0.000738\n",
      "359) 281                            0.000738\n",
      "360) 123                            0.000737\n",
      "361) 345                            0.000735\n",
      "362) 185                            0.000733\n",
      "363) 74                             0.000733\n",
      "364) 133                            0.000731\n",
      "365) 145                            0.000730\n",
      "366) 383                            0.000726\n",
      "367) 339                            0.000726\n",
      "368) 310                            0.000722\n",
      "369) 486                            0.000721\n",
      "370) 164                            0.000717\n",
      "371) 454                            0.000714\n",
      "372) 173                            0.000711\n",
      "373) 75                             0.000708\n",
      "374) 317                            0.000706\n",
      "375) 141                            0.000705\n",
      "376) 427                            0.000704\n",
      "377) 363                            0.000703\n",
      "378) 278                            0.000702\n",
      "379) 69                             0.000702\n",
      "380) 263                            0.000701\n",
      "381) 229                            0.000701\n",
      "382) 54                             0.000698\n",
      "383) 210                            0.000698\n",
      "384) 441                            0.000693\n",
      "385) 241                            0.000691\n",
      "386) 243                            0.000689\n",
      "387) 478                            0.000686\n",
      "388) 473                            0.000683\n",
      "389) 204                            0.000682\n",
      "390) 371                            0.000681\n",
      "391) 187                            0.000679\n",
      "392) 257                            0.000676\n",
      "393) 225                            0.000674\n",
      "394) 296                            0.000672\n",
      "395) 350                            0.000671\n",
      "396) 156                            0.000670\n",
      "397) 95                             0.000670\n",
      "398) 177                            0.000670\n",
      "399) 82                             0.000669\n",
      "400) 336                            0.000666\n",
      "401) 429                            0.000665\n",
      "402) 307                            0.000665\n",
      "403) 346                            0.000663\n",
      "404) 233                            0.000663\n",
      "405) 140                            0.000662\n",
      "406) 403                            0.000660\n",
      "407) 456                            0.000659\n",
      "408) 188                            0.000658\n",
      "409) 127                            0.000657\n",
      "410) 405                            0.000656\n",
      "411) 152                            0.000654\n",
      "412) 466                            0.000652\n",
      "413) 138                            0.000648\n",
      "414) 373                            0.000648\n",
      "415) 224                            0.000647\n",
      "416) 408                            0.000643\n",
      "417) 232                            0.000639\n",
      "418) 137                            0.000637\n",
      "419) 326                            0.000635\n",
      "420) 143                            0.000632\n",
      "421) 292                            0.000631\n",
      "422) 277                            0.000629\n",
      "423) 136                            0.000627\n",
      "424) 85                             0.000624\n",
      "425) 88                             0.000624\n",
      "426) 414                            0.000623\n",
      "427) 364                            0.000622\n",
      "428) 365                            0.000620\n",
      "429) 226                            0.000619\n",
      "430) 360                            0.000617\n",
      "431) 300                            0.000616\n",
      "432) 118                            0.000612\n",
      "433) 206                            0.000612\n",
      "434) 302                            0.000612\n",
      "435) 368                            0.000607\n",
      "436) 432                            0.000604\n",
      "437) 149                            0.000603\n",
      "438) 329                            0.000594\n",
      "439) 297                            0.000593\n",
      "440) 348                            0.000593\n",
      "441) 392                            0.000592\n",
      "442) 377                            0.000592\n",
      "443) 415                            0.000590\n",
      "444) 139                            0.000589\n",
      "445) 190                            0.000588\n",
      "446) 201                            0.000586\n",
      "447) 498                            0.000586\n",
      "448) 191                            0.000584\n",
      "449) 340                            0.000582\n",
      "450) 197                            0.000580\n",
      "451) 247                            0.000580\n",
      "452) 333                            0.000580\n",
      "453) 347                            0.000577\n",
      "454) 260                            0.000577\n",
      "455) 59                             0.000576\n",
      "456) 130                            0.000573\n",
      "457) 196                            0.000571\n",
      "458) 357                            0.000567\n",
      "459) 245                            0.000565\n",
      "460) 433                            0.000563\n",
      "461) 275                            0.000561\n",
      "462) 412                            0.000561\n",
      "463) 255                            0.000561\n",
      "464) 97                             0.000559\n",
      "465) 442                            0.000554\n",
      "466) 372                            0.000553\n",
      "467) 378                            0.000551\n",
      "468) 250                            0.000550\n",
      "469) 64                             0.000550\n",
      "470) 109                            0.000548\n",
      "471) 393                            0.000548\n",
      "472) 213                            0.000547\n",
      "473) 440                            0.000539\n",
      "474) 343                            0.000538\n",
      "475) 395                            0.000537\n",
      "476) 305                            0.000536\n",
      "477) 342                            0.000533\n",
      "478) 324                            0.000531\n",
      "479) 397                            0.000526\n",
      "480) 214                            0.000522\n",
      "481) 205                            0.000521\n",
      "482) 379                            0.000520\n",
      "483) 391                            0.000517\n",
      "484) 65                             0.000515\n",
      "485) 389                            0.000515\n",
      "486) 384                            0.000515\n",
      "487) 163                            0.000507\n",
      "488) 304                            0.000497\n",
      "489) 366                            0.000495\n",
      "490) 424                            0.000494\n",
      "491) 193                            0.000486\n",
      "492) 207                            0.000483\n",
      "493) 358                            0.000476\n",
      "494) 169                            0.000476\n",
      "495) 425                            0.000473\n",
      "496) 239                            0.000469\n",
      "497) 334                            0.000453\n",
      "498) 157                            0.000446\n",
      "499) 380                            0.000433\n",
      "500) 400                            0.000413\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 500 is out of bounds for axis 0 with size 500",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-665614754bd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     print(\"%2d) %-*s %f\" % (f + 1, 30, \n\u001b[1;32m----> 6\u001b[1;33m                             \u001b[0mfeat_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m                             importances[indices[f]]))\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 500 is out of bounds for axis 0 with size 500"
     ]
    }
   ],
   "source": [
    "feat_labels = range(X_train_pca.shape[1])\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            feat_labels[indices[f]], \n",
    "                            importances[indices[f]]))\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(X_train_pca.shape[1]), \n",
    "        importances[indices],\n",
    "        align='center')\n",
    "\n",
    "plt.xticks(range(X_train_pca.shape[1]), \n",
    "           feat_labels[indices], rotation=90)\n",
    "plt.xlim([-1, X_train_pca.shape[1]])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/04_09.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning multiple linear regression with polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T19:01:52.114282Z",
     "start_time": "2021-05-10T19:01:52.101352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T19:17:03.520059Z",
     "start_time": "2021-05-10T19:17:03.508756Z"
    }
   },
   "outputs": [],
   "source": [
    "def r_2_score(y_true, y_pred):\n",
    "    from tensorflow.keras import backend as K\n",
    "    RSS =  K.sum(K.square( y_true- y_pred ))\n",
    "    TSS = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1. - RSS/(TSS) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T19:20:01.518247Z",
     "start_time": "2021-05-10T19:20:01.512299Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T19:20:04.457314Z",
     "start_time": "2021-05-10T19:20:04.397782Z"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(X_train_pca.shape[1],))\n",
    "dense_layer_1 = Dense(100, activation='relu')(input_layer)\n",
    "dense_layer_2 = Dense(50, activation='relu')(dense_layer_1)\n",
    "dense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\n",
    "output = Dense(1)(dense_layer_3)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\",\"mean_squared_logarithmic_error\",det_coeff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T19:42:48.266939Z",
     "start_time": "2021-05-10T19:20:09.669230Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 27502.4492 - mean_squared_error: 27502.4492 - mean_squared_logarithmic_error: 1.2241 - det_coeff: -inf - val_loss: 25228.3965 - val_mean_squared_error: 25228.3965 - val_mean_squared_logarithmic_error: 0.9755 - val_det_coeff: -inf\n",
      "Epoch 2/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 23168.6055 - mean_squared_error: 23168.6055 - mean_squared_logarithmic_error: 1.0271 - det_coeff: -inf - val_loss: 24931.4199 - val_mean_squared_error: 24931.4199 - val_mean_squared_logarithmic_error: 1.1732 - val_det_coeff: -infn_squared_error: 22650.7832 - mean_squared_logarithmic_error: 1.0234 - det_c - ETA: 3s - loss: 22628.4902 - mean_squared_error: 2\n",
      "Epoch 3/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 21653.6055 - mean_squared_error: 21653.6055 - mean_squared_logarithmic_error: 0.9881 - det_coeff: -inf - val_loss: 22447.9824 - val_mean_squared_error: 22447.9824 - val_mean_squared_logarithmic_error: 1.1357 - val_det_coeff: -inf\n",
      "Epoch 4/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 20732.7422 - mean_squared_error: 20732.7422 - mean_squared_logarithmic_error: 0.9645 - det_coeff: -inf - val_loss: 22273.5488 - val_mean_squared_error: 22273.5488 - val_mean_squared_logarithmic_error: 1.1145 - val_det_coeff: -inf\n",
      "Epoch 5/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 20257.8281 - mean_squared_error: 20257.8281 - mean_squared_logarithmic_error: 0.9410 - det_coeff: -inf - val_loss: 22732.7266 - val_mean_squared_error: 22732.7266 - val_mean_squared_logarithmic_error: 0.8842 - val_det_coeff: -inf\n",
      "Epoch 6/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 19489.2051 - mean_squared_error: 19489.2051 - mean_squared_logarithmic_error: 0.9200 - det_coeff: -inf - val_loss: 20481.5527 - val_mean_squared_error: 20481.5527 - val_mean_squared_logarithmic_error: 1.0290 - val_det_coeff: -infloss: 19313.5215 - mean_squared_error: 19313.5215 - mean_squared_logarithmic_error: 0.8959 - det_coe - ETA: 6s - loss: 19245.2598 - mean_squared_error: 19245.2598 - mean_squared_logarithmic_e - ETA: 4s - loss: 18830.4395 - mean_squared_error: 18830.4395 - mean_squared_logarithmic - ETA: 3s - loss: 19152.1680 - mean_squared_error: 19152.1680 - mean_squared_loga - ETA: 1s - loss: 19111.3945 - mean_squared_error: 19111.3945 - mean_squared_logarithmic_error: 0. - ETA: 0s - loss: 19481.2285 - mean_squared_error: 19481.2285 - mean_squared_logarithmic_error: 0.9197 - det_coeff: -in\n",
      "Epoch 7/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 18924.6309 - mean_squared_error: 18924.6309 - mean_squared_logarithmic_error: 0.9216 - det_coeff: -inf - val_loss: 20580.3125 - val_mean_squared_error: 20580.3125 - val_mean_squared_logarithmic_error: 0.9293 - val_det_coeff: -infss: 19170.1387 - mean_squared_error: 19170.1387 - mean_squared_logarithmic_error: 0.9276 - det_coef - ETA: 1s - loss: 18959.9609 - mean_squared_error: 18959.9609 - mean_squared_logarithmic_error: 0.9264 - de - ETA: 0s - loss: 19001.2734 - mean_squared_error: 19001.2734 - mean_squared_logarithmic_error: 0.9256 - det_coeff:  - ETA: 0s - loss: 18886.9980 - mean_squared_error: 18886.9980 - mean_squared_logarithmic_error: 0.9250 - det_coeff: - - ETA: 0s - loss: 18792.2559 - mean_squared_error: 18792.2559 - mean_squared_logarithmic_error: 0.9222 - det_coeff - ETA: 0s - loss: 18855.8965 - mean_squared_error: 18855.8965 - mean_squared_logarithmic_error: 0.9201 - det_coeff:\n",
      "Epoch 8/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 18490.8379 - mean_squared_error: 18490.8379 - mean_squared_logarithmic_error: 0.8992 - det_coeff: -inf - val_loss: 21092.4824 - val_mean_squared_error: 21092.4824 - val_mean_squared_logarithmic_error: 1.0076 - val_det_coeff: -inf023 - mean_sq - ETA: 1s - loss: 18509.7207 - mean_squared_error: 18509.7207 - mean_squared_logar\n",
      "Epoch 9/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 18131.6758 - mean_squared_error: 18131.6758 - mean_squared_logarithmic_error: 0.8918 - det_coeff: -inf - val_loss: 18996.1172 - val_mean_squared_error: 18996.1172 - val_mean_squared_logarithmic_error: 0.9336 - val_det_coeff: -inf\n",
      "Epoch 10/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 17704.5137 - mean_squared_error: 17704.5137 - mean_squared_logarithmic_error: 0.8714 - det_coeff: -inf - val_loss: 20731.7969 - val_mean_squared_error: 20731.7969 - val_mean_squared_logarithmic_error: 1.1087 - val_det_coeff: -infsquared_error: 17962.3477 - mean_squared_logarithmic_error: 0.8668 - det_coeff: -i - ETA: 2s - loss: 18056.7285 - mean_squared_error: 18056.7285 - mean_squared_logarithmic_error: 0. - ETA: 0s - loss: 17948.0156 - mean_squared_error: 17948.0156 - mean_squared_logarithmic_error: 0.8699 - det_coeff: - ETA: 0s - loss: 17829.6719 - mean_squared_error: 17829.6719 - mean_squared_logarithmic_error: 0.8691 - de\n",
      "Epoch 11/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 17624.5039 - mean_squared_error: 17624.5039 - mean_squared_logarithmic_error: 0.8629 - det_coeff: -inf - val_loss: 18923.8086 - val_mean_squared_error: 18923.8086 - val_mean_squared_logarithmic_error: 0.8354 - val_det_coeff: -infsquared_error: 17560.2539 - mean_squared_logarithmic_error: 0.8576 - det_coeff: - - ETA: 1s - loss: 17745.0371 - mean_squared_error: 17745.0371 - mean_squared_logarithmic_error: 0.8607 - det_coeff: - ETA: 0s - loss: 17973.9141 - mean_squared_error: 17973.9141 - mean_squared_logarithmic_error: 0.863\n",
      "Epoch 12/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 17042.7656 - mean_squared_error: 17042.7656 - mean_squared_logarithmic_error: 0.8509 - det_coeff: -inf - val_loss: 18518.2773 - val_mean_squared_error: 18518.2773 - val_mean_squared_logarithmic_error: 0.8770 - val_det_coeff: -infquared_error: 17504.5195 - mean_squared_logarithmic_error: 0.8513 - ETA: 5s - loss: 17349.1484 - mean_squared_error: 17349.1484 - mean_squared_l - ETA: 3s - loss: 16788.6719 - mean_squared_error: 16788.6719 - mean_squared_logarithmic_error: 0.8498 - d - ETA: 2s - loss: 16917.2520 - mean_squared_error: 16917.2520 - mean_squa\n",
      "Epoch 13/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 16884.4707 - mean_squared_error: 16884.4707 - mean_squared_logarithmic_error: 0.8426 - det_coeff: -inf - val_loss: 19154.5000 - val_mean_squared_error: 19154.5000 - val_mean_squared_logarithmic_error: 0.8066 - val_det_coeff: -infETA: 1s - loss: 16707.8008 - mean_squared_error: 16707.8008 - mean_squared_logarithmic_error: - ETA: 0s - loss: 16901.0098 - mean_squared_error: 16901.0098 - mean_squared_logarithmic_error: 0.8415 - det_coeff: -i\n",
      "Epoch 14/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 16679.7344 - mean_squared_error: 16679.7344 - mean_squared_logarithmic_error: 0.8315 - det_coeff: -inf - val_loss: 17984.8359 - val_mean_squared_error: 17984.8359 - val_mean_squared_logarithmic_error: 0.7637 - val_det_coeff: -infred_error: 16758.8008 -  - ETA: 0s - loss: 16770.7500 - mean_squared_error: 16770.7500 - mean_squared_logarithmic_error: 0.8323 - det_\n",
      "Epoch 15/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 16462.9766 - mean_squared_error: 16462.9766 - mean_squared_logarithmic_error: 0.8283 - det_coeff: -inf - val_loss: 17721.0469 - val_mean_squared_error: 17721.0469 - val_mean_squared_logarithmic_error: 1.0161 - val_det_coeff: -inf\n",
      "Epoch 16/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 16329.8203 - mean_squared_error: 16329.8203 - mean_squared_logarithmic_error: 0.8316 - det_coeff: -inf - val_loss: 17391.5215 - val_mean_squared_error: 17391.5215 - val_mean_squared_logarithmic_error: 0.8624 - val_det_coeff: -inf mean_squared_error: 16798.3809 - mean_squared_logarithmic_error: 0.8540 - det_co - ETA: 8s - loss: 15947.4736 - mean_squared_error: 15947.4736  - ETA: 5s - loss: 16672.9746 - mean_squared_error: 16672.9746 - mean_squared_logarithmic_error: 0.8261 - det_coef - ETA: 4s - loss: 16593.8066 - mean_squared_error: 16593.8066 - mean_squared_logarithmic - ETA: 2s - loss: 16442.7324 - mean_squared_error: 16442.7324 - m\n",
      "Epoch 17/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 15682.1094 - mean_squared_error: 15682.1094 - mean_squared_logarithmic_error: 0.8192 - det_coeff: -inf - val_loss: 16820.3477 - val_mean_squared_error: 16820.3477 - val_mean_squared_logarithmic_error: 0.8263 - val_det_coeff: -inf.2607 - mean_squ - ETA: 2s - loss: 16075.3301 - mean_squared_error: 16075.3301\n",
      "Epoch 18/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 15948.5361 - mean_squared_error: 15948.5361 - mean_squared_logarithmic_error: 0.8214 - det_coeff: -inf - val_loss: 17265.6953 - val_mean_squared_error: 17265.6953 - val_mean_squared_logarithmic_error: 0.8601 - val_det_coeff: -infoss: 16248.5527 - mean_squared_error: 16248.5527 - mean_squared_logarithmic_ - ETA: 5s - loss: 15932.3936 - mean_squared_error: 15932.3936 - mean_squared_logarithmic_error: 0.8291 - det_co - ETA: 5s - loss: 16023.9424 - mean_squared_error: 16023.9424 - mean_squared_logarithmic_error: 0.8274 - det_coeff: -i - ETA: 5s - \n",
      "Epoch 19/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 15544.9395 - mean_squared_error: 15544.9395 - mean_squared_logarithmic_error: 0.8061 - det_coeff: -inf - val_loss: 17520.9531 - val_mean_squared_error: 17520.9531 - val_mean_squared_logarithmic_error: 0.9184 - val_det_coeff: -inf5104.7354 - mean_squared - ETA: 1s - loss: 15752.5400 - mean_squared_error: 15752.5400 - mean_squared_logarithmic_error: \n",
      "Epoch 20/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 15304.5850 - mean_squared_error: 15304.5850 - mean_squared_logarithmic_error: 0.8106 - det_coeff: -inf - val_loss: 17545.3711 - val_mean_squared_error: 17545.3711 - val_mean_squared_logarithmic_error: 1.0540 - val_det_coeff: -inf_squared_logarithmic_error: 0.7951 - det_coeff: - ETA: 3s - loss: 14896.5850 - mean_squared_error: 14896.5850 - ETA: 0s - loss: 14909.0537 - mean_squared_error: 14909.0537 - mean_squared_logarithmic_error: 0.804\n",
      "Epoch 21/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 15118.3291 - mean_squared_error: 15118.3291 - mean_squared_logarithmic_error: 0.8075 - det_coeff: -inf - val_loss: 16808.9043 - val_mean_squared_error: 16808.9043 - val_mean_squared_logarithmic_error: 0.9864 - val_det_coeff: -inf\n",
      "Epoch 22/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 15010.0215 - mean_squared_error: 15010.0215 - mean_squared_logarithmic_error: 0.8006 - det_coeff: -inf - val_loss: 16347.0127 - val_mean_squared_error: 16347.0127 - val_mean_squared_logarithmic_error: 0.8461 - val_det_coeff: -inf656.2891 - mean_squared_logarithmic - ETA: 5s - loss: 1\n",
      "Epoch 23/100\n",
      "12250/12250 [==============================] - 12s 1ms/step - loss: 14688.3477 - mean_squared_error: 14688.3477 - mean_squared_logarithmic_error: 0.7938 - det_coeff: -inf - val_loss: 17766.3574 - val_mean_squared_error: 17766.3574 - val_mean_squared_logarithmic_error: 0.8179 - val_det_coeff: -infrror: 14479.2051 - mean_squared_logarithmic_error: 0.7938 - \n",
      "Epoch 24/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 14472.7656 - mean_squared_error: 14472.7656 - mean_squared_logarithmic_error: 0.7825 - det_coeff: -inf - val_loss: 16781.3184 - val_mean_squared_error: 16781.3184 - val_mean_squared_logarithmic_error: 0.8077 - val_det_coeff: -inf80.1924\n",
      "Epoch 25/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 14477.1162 - mean_squared_error: 14477.1162 - mean_squared_logarithmic_error: 0.7828 - det_coeff: -inf - val_loss: 16238.1533 - val_mean_squared_error: 16238.1533 - val_mean_squared_logarithmic_error: 0.8128 - val_det_coeff: -inf\n",
      "Epoch 26/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 14252.3027 - mean_squared_error: 14252.3027 - mean_squared_logarithmic_error: 0.7905 - det_coeff: -inf - val_loss: 16083.0869 - val_mean_squared_error: 16083.0869 - val_mean_squared_logarithmic_error: 0.8429 - val_det_coeff: -infn_squared_error: 1470\n",
      "Epoch 27/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 13803.9590 - mean_squared_error: 13803.9590 - mean_squared_logarithmic_error: 0.7733 - det_coeff: -inf - val_loss: 18212.3320 - val_mean_squared_error: 18212.3320 - val_mean_squared_logarithmic_error: 1.0120 - val_det_coeff: -inf_squared_error: 13352.1221 - mean_squared_logarithmic_error: 0.7719 - det_coeff: - ETA: 3s - loss: 13420.5752 - mean_squared_error\n",
      "Epoch 28/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 13745.7607 - mean_squared_error: 13745.7607 - mean_squared_logarithmic_error: 0.7856 - det_coeff: -inf - val_loss: 16963.2598 - val_mean_squared_error: 16963.2598 - val_mean_squared_logarithmic_error: 0.9141 - val_det_coeff: -inf: 13893.4189 - mean_squared_error: 13893.4189 - mean_squared_logarithmic_error: 0.7\n",
      "Epoch 29/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 13469.5801 - mean_squared_error: 13469.5801 - mean_squared_logarithmic_error: 0.7718 - det_coeff: -inf - val_loss: 16738.9980 - val_mean_squared_error: 16738.9980 - val_mean_squared_logarithmic_error: 0.8905 - val_det_coeff: -inf6 - mean_squared_logarithmic_error: 0.7716 - det_coeff - ETA: 6s - loss: 12617.4023 - mean_squ - ETA: 2s - loss: 13630.0322 - mean_squared_error: 13630.0322 - mean_squared_logarithmic_error: 0.7738 - det_coeff:  - ETA: 2s - loss: 13602.2061 - mean_squared_error: 13602.2061 - mean_squared_logarithmic_error: 0.7725 - det_co - ETA: 1s - loss: 13368.8672 - mean_squared_error: 13368.8672 - mean_squared_logarit\n",
      "Epoch 30/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 13484.0449 - mean_squared_error: 13484.0449 - mean_squared_logarithmic_error: 0.7792 - det_coeff: -inf - val_loss: 15721.9248 - val_mean_squared_error: 15721.9248 - val_mean_squared_logarithmic_error: 0.8149 - val_det_coeff: -inf_squared_error: 13423.6045 - mean_squared_l - ETA: 5s - loss: 13121.8105 - mean_squared_error: 13121.8105 - mean_squared_logarithmic_er - ETA: 3s - loss: 13270.7939 - mean_squared_error: 13270.7939 - mean_squared_logarithmic_e - ETA: 2s - loss: 13418.4004 - mean_squared_error: 13418.4004 - mean_squared_logarithmic_erro - ETA: 1s - loss: 13591.0771 - mean_squared_error: 13591.0771 - mean_squared_logarithmic_error: 0.7\n",
      "Epoch 31/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 13312.4189 - mean_squared_error: 13312.4189 - mean_squared_logarithmic_error: 0.7686 - det_coeff: -inf - val_loss: 15679.4678 - val_mean_squared_error: 15679.4678 - val_mean_squared_logarithmic_error: 0.7931 - val_det_coeff: -inf\n",
      "Epoch 32/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 13118.9668 - mean_squared_error: 13118.9668 - mean_squared_logarithmic_error: 0.7731 - det_coeff: -inf - val_loss: 17877.8496 - val_mean_squared_error: 17877.8496 - val_mean_squared_logarithmic_error: 0.9701 - val_det_coeff: -infquared_error: 13322.0537 - mean_squared_logarithmic_error: 0.7650 - det_c - ETA: 3s - loss: 13343.4570 - mean_squared_error: 13343.4570 - mean_squared_logarithmic_error: 0.7715 - det_coeff: - ETA: 3s - loss: 13196.0889 - mean_squared_error: 13196.\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12250/12250 [==============================] - 14s 1ms/step - loss: 12940.5039 - mean_squared_error: 12940.5039 - mean_squared_logarithmic_error: 0.7619 - det_coeff: -inf - val_loss: 16808.1348 - val_mean_squared_error: 16808.1348 - val_mean_squared_logarithmic_error: 0.8210 - val_det_coeff: -inf\n",
      "Epoch 34/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 12568.7129 - mean_squared_error: 12568.7129 - mean_squared_logarithmic_error: 0.7597 - det_coeff: -inf - val_loss: 15230.9590 - val_mean_squared_error: 15230.9590 - val_mean_squared_logarithmic_error: 0.8207 - val_det_coeff: -infr: 12721.4277 - mean_squared_logarithmic_error: 0.7 - ETA: 2s - loss: 12274.6953 - mean_squared_error: 12274.6953 - mean_squared_logarithmic_error: 0.7621 - det_coe - ETA: 2s - loss: 12402.8125 - mean_squared_error: 12402.8125 - mean_squared_\n",
      "Epoch 35/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 12562.4775 - mean_squared_error: 12562.4775 - mean_squared_logarithmic_error: 0.7620 - det_coeff: -inf - val_loss: 14392.3203 - val_mean_squared_error: 14392.3203 - val_mean_squared_logarithmic_error: 0.8078 - val_det_coeff: -inf\n",
      "Epoch 36/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 12418.1484 - mean_squared_error: 12418.1484 - mean_squared_logarithmic_error: 0.7510 - det_coeff: -inf - val_loss: 14120.7207 - val_mean_squared_error: 14120.7207 - val_mean_squared_logarithmic_error: 0.8669 - val_det_coeff: -inf\n",
      "Epoch 37/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 12177.0869 - mean_squared_error: 12177.0869 - mean_squared_logarithmic_error: 0.7671 - det_coeff: -inf - val_loss: 15172.0703 - val_mean_squared_error: 15172.0703 - val_mean_squared_logarithmic_error: 0.7707 - val_det_coeff: -infared_logarithmic_error: 0.7693 - det_co - ETA: 7s - loss: 11646.5820 - ETA: 2s - loss: 12408.3779 - mean_squared_error: 12408.3779 - mean_squa - ETA: 0s - loss: 12200.0420 - mean_squared_error: 12200.0420 - mean_squared_logarithmic_error: 0.7677 - det_coeff: -\n",
      "Epoch 38/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 11941.8164 - mean_squared_error: 11941.8164 - mean_squared_logarithmic_error: 0.7485 - det_coeff: -inf - val_loss: 14693.2969 - val_mean_squared_error: 14693.2969 - val_mean_squared_logarithmic_error: 0.7858 - val_det_coeff: -infA: 4s - loss: 11912.7832 - mean_squa\n",
      "Epoch 39/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 11925.2031 - mean_squared_error: 11925.2031 - mean_squared_logarithmic_error: 0.7522 - det_coeff: -inf - val_loss: 14899.6084 - val_mean_squared_error: 14899.6084 - val_mean_squared_logarithmic_error: 0.7709 - val_det_coeff: -inf01.4434 - mean_squ\n",
      "Epoch 40/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 11677.3965 - mean_squared_error: 11677.3965 - mean_squared_logarithmic_error: 0.7396 - det_coeff: -inf - val_loss: 15724.3506 - val_mean_squared_error: 15724.3506 - val_mean_squared_logarithmic_error: 0.9254 - val_det_coeff: -inf\n",
      "Epoch 41/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 11645.2373 - mean_squared_error: 11645.2373 - mean_squared_logarithmic_error: 0.7398 - det_coeff: -inf - val_loss: 14544.7715 - val_mean_squared_error: 14544.7715 - val_mean_squared_logarithmic_error: 0.8295 - val_det_coeff: -inf\n",
      "Epoch 42/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 11322.1748 - mean_squared_error: 11322.1748 - mean_squared_logarithmic_error: 0.7473 - det_coeff: -inf - val_loss: 15496.0107 - val_mean_squared_error: 15496.0107 - val_mean_squared_logarithmic_error: 0.8097 - val_det_coeff: -inf\n",
      "Epoch 43/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 11289.7373 - mean_squared_error: 11289.7373 - mean_squared_logarithmic_error: 0.7423 - det_coeff: -inf - val_loss: 14979.8018 - val_mean_squared_error: 14979.8018 - val_mean_squared_logarithmic_error: 0.8392 - val_det_coeff: -inf\n",
      "Epoch 44/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 11165.1504 - mean_squared_error: 11165.1504 - mean_squared_logarithmic_error: 0.7369 - det_coeff: -inf - val_loss: 14669.8008 - val_mean_squared_error: 14669.8008 - val_mean_squared_logarithmic_error: 0.7724 - val_det_coeff: -inf\n",
      "Epoch 45/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 11148.7910 - mean_squared_error: 11148.7910 - mean_squared_logarithmic_error: 0.7380 - det_coeff: -inf - val_loss: 14110.4346 - val_mean_squared_error: 14110.4346 - val_mean_squared_logarithmic_error: 0.8306 - val_det_coeff: -inf\n",
      "Epoch 46/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 10938.3828 - mean_squared_error: 10938.3828 - mean_squared_logarithmic_error: 0.7316 - det_coeff: -inf - val_loss: 13844.6768 - val_mean_squared_error: 13844.6768 - val_mean_squared_logarithmic_error: 0.7954 - val_det_coeff: -inf\n",
      "Epoch 47/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 10606.5137 - mean_squared_error: 10606.5137 - mean_squared_logarithmic_error: 0.7221 - det_coeff: -inf - val_loss: 16168.8779 - val_mean_squared_error: 16168.8779 - val_mean_squared_logarithmic_error: 0.8213 - val_det_coeff: -inf\n",
      "Epoch 48/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 10555.4189 - mean_squared_error: 10555.4189 - mean_squared_logarithmic_error: 0.7226 - det_coeff: -inf - val_loss: 14004.8945 - val_mean_squared_error: 14004.8945 - val_mean_squared_logarithmic_error: 0.7421 - val_det_coeff: -inf\n",
      "Epoch 49/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 10457.1240 - mean_squared_error: 10457.1240 - mean_squared_logarithmic_error: 0.7284 - det_coeff: -inf - val_loss: 15569.9141 - val_mean_squared_error: 15569.9141 - val_mean_squared_logarithmic_error: 0.8653 - val_det_coeff: -infror: 10132.1465 - mean_squared_logarithmic_error: 0.704 - ETA:\n",
      "Epoch 50/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 10244.0752 - mean_squared_error: 10244.0752 - mean_squared_logarithmic_error: 0.7163 - det_coeff: -inf - val_loss: 14942.9961 - val_mean_squared_error: 14942.9961 - val_mean_squared_logarithmic_error: 0.7928 - val_det_coeff: -inf mean_squared_logarithmic_error: 0.7015 - det_coeff: - ETA: 3s - loss: 9876.6846 - mean_squared_error: 9876.6846 - mean_squared_logarithmic_error: 0.7021 - det_coeff:  - ETA: 2s - loss: 9935.3701 - mean_squared_error: 9935.3701 -\n",
      "Epoch 51/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 10388.6631 - mean_squared_error: 10388.6631 - mean_squared_logarithmic_error: 0.7275 - det_coeff: -inf - val_loss: 14086.8711 - val_mean_squared_error: 14086.8711 - val_mean_squared_logarithmic_error: 0.8597 - val_det_coeff: -inf\n",
      "Epoch 52/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 10262.5635 - mean_squared_error: 10262.5635 - mean_squared_logarithmic_error: 0.7208 - det_coeff: -inf - val_loss: 14675.6191 - val_mean_squared_error: 14675.6191 - val_mean_squared_logarithmic_error: 0.7461 - val_det_coeff: -inf\n",
      "Epoch 53/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 10264.8809 - mean_squared_error: 10264.8809 - mean_squared_logarithmic_error: 0.7086 - det_coeff: -inf - val_loss: 14411.1631 - val_mean_squared_error: 14411.1631 - val_mean_squared_logarithmic_error: 0.7821 - val_det_coeff: -inf\n",
      "Epoch 54/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 10041.9941 - mean_squared_error: 10041.9941 - mean_squared_logarithmic_error: 0.6973 - det_coeff: -inf - val_loss: 15113.4658 - val_mean_squared_error: 15113.4658 - val_mean_squared_logarithmic_error: 0.7725 - val_det_coeff: -inf\n",
      "Epoch 55/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 9815.1094 - mean_squared_error: 9815.1094 - mean_squared_logarithmic_error: 0.7084 - det_coeff: -inf - val_loss: 13883.5518 - val_mean_squared_error: 13883.5518 - val_mean_squared_logarithmic_error: 0.7527 - val_det_coeff: -inf\n",
      "Epoch 56/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 9792.1670 - mean_squared_error: 9792.1670 - mean_squared_logarithmic_error: 0.6971 - det_coeff: -inf - val_loss: 15096.0840 - val_mean_squared_error: 15096.0840 - val_mean_squared_logarithmic_error: 0.8600 - val_det_coeff: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 9886.0420 - mean_squared_error: 9886.0420 - mean_squared_logarithmic_error: 0.6970 - det_coeff: -inf - val_loss: 14483.3555 - val_mean_squared_error: 14483.3555 - val_mean_squared_logarithmic_error: 0.8303 - val_det_coeff: -inf\n",
      "Epoch 58/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 9623.2539 - mean_squared_error: 9623.2539 - mean_squared_logarithmic_error: 0.7022 - det_coeff: -inf - val_loss: 13866.3516 - val_mean_squared_error: 13866.3516 - val_mean_squared_logarithmic_error: 0.7694 - val_det_coeff: -inf\n",
      "Epoch 59/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 9467.2676 - mean_squared_error: 9467.2676 - mean_squared_logarithmic_error: 0.6957 - det_coeff: -inf - val_loss: 14260.2363 - val_mean_squared_error: 14260.2363 - val_mean_squared_logarithmic_error: 0.7792 - val_det_coeff: -inf\n",
      "Epoch 60/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 9474.6543 - mean_squared_error: 9474.6543 - mean_squared_logarithmic_error: 0.6905 - det_coeff: -inf - val_loss: 13717.5469 - val_mean_squared_error: 13717.5469 - val_mean_squared_logarithmic_error: 0.7281 - val_det_coeff: -inf\n",
      "Epoch 61/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 9254.5186 - mean_squared_error: 9254.5186 - mean_squared_logarithmic_error: 0.7023 - det_coeff: -inf - val_loss: 13927.9883 - val_mean_squared_error: 13927.9883 - val_mean_squared_logarithmic_error: 0.7768 - val_det_coeff: -inf\n",
      "Epoch 62/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 9296.4473 - mean_squared_error: 9296.4473 - mean_squared_logarithmic_error: 0.6936 - det_coeff: -inf - val_loss: 13680.6982 - val_mean_squared_error: 13680.6982 - val_mean_squared_logarithmic_error: 0.7408 - val_det_coeff: -infTA: 4s - loss: 8830.\n",
      "Epoch 63/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 9311.2480 - mean_squared_error: 9311.2480 - mean_squared_logarithmic_error: 0.7013 - det_coeff: -inf - val_loss: 13563.3447 - val_mean_squared_error: 13563.3447 - val_mean_squared_logarithmic_error: 0.7477 - val_det_coeff: -infss: 8956.4189 - mean_squared_error: 8956.4189 - mean_squared_logarithmic_error:  - ETA: 3s - loss: 9059.2197 - mean_squared_error: 9059.2197 - mean_squared_logarithmic_error: 0.7037 - det_coeff: -in - ETA: 3s - loss: 9037.9727 - mean_squared_error: 903\n",
      "Epoch 64/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 9210.5342 - mean_squared_error: 9210.5342 - mean_squared_logarithmic_error: 0.6878 - det_coeff: -inf - val_loss: 14102.5781 - val_mean_squared_error: 14102.5781 - val_mean_squared_logarithmic_error: 0.7265 - val_det_coeff: -inf\n",
      "Epoch 65/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 9184.7695 - mean_squared_error: 9184.7695 - mean_squared_logarithmic_error: 0.6942 - det_coeff: -inf - val_loss: 13829.2021 - val_mean_squared_error: 13829.2021 - val_mean_squared_logarithmic_error: 0.7872 - val_det_coeff: -inf\n",
      "Epoch 66/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 8938.2188 - mean_squared_error: 8938.2188 - mean_squared_logarithmic_error: 0.6877 - det_coeff: -inf - val_loss: 13522.3730 - val_mean_squared_error: 13522.3730 - val_mean_squared_logarithmic_error: 0.8291 - val_det_coeff: -inf\n",
      "Epoch 67/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 8985.7051 - mean_squared_error: 8985.7051 - mean_squared_logarithmic_error: 0.6951 - det_coeff: -inf - val_loss: 14347.9570 - val_mean_squared_error: 14347.9570 - val_mean_squared_logarithmic_error: 0.7824 - val_det_coeff: -inf\n",
      "Epoch 68/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 8815.6709 - mean_squared_error: 8815.6709 - mean_squared_logarithmic_error: 0.6819 - det_coeff: -inf - val_loss: 14006.9990 - val_mean_squared_error: 14006.9990 - val_mean_squared_logarithmic_error: 0.8299 - val_det_coeff: -infn_squared_logarithmic_erro\n",
      "Epoch 69/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 8687.8945 - mean_squared_error: 8687.8945 - mean_squared_logarithmic_error: 0.6788 - det_coeff: -inf - val_loss: 14023.5371 - val_mean_squared_error: 14023.5371 - val_mean_squared_logarithmic_error: 0.7706 - val_det_coeff: -inf37.1270 - mean_squared_error: 8937.1270 - mean_squared - ETA: 5s - loss: 8794.7324 - mean_squared_error: 8794.7324 - mean_squared_logarithmic_erro - ETA: 3s - loss: 9086.8906 - mean_squared_error\n",
      "Epoch 70/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 8733.2393 - mean_squared_error: 8733.2393 - mean_squared_logarithmic_error: 0.6829 - det_coeff: -inf - val_loss: 13349.2783 - val_mean_squared_error: 13349.2783 - val_mean_squared_logarithmic_error: 0.7846 - val_det_coeff: -infrror: 8717.3418 - mean_squared_logarithmic_error: 0.6831 - det_co\n",
      "Epoch 71/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 8495.4082 - mean_squared_error: 8495.4082 - mean_squared_logarithmic_error: 0.6876 - det_coeff: -inf - val_loss: 13489.6230 - val_mean_squared_error: 13489.6230 - val_mean_squared_logarithmic_error: 0.7829 - val_det_coeff: -inf\n",
      "Epoch 72/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 8525.2988 - mean_squared_error: 8525.2988 - mean_squared_logarithmic_error: 0.6815 - det_coeff: -inf - val_loss: 15325.9463 - val_mean_squared_error: 15325.9463 - val_mean_squared_logarithmic_error: 0.8171 - val_det_coeff: -infsquared_error: 8248.9238 - mean_squared_logar - ETA: 7s - loss: - ETA: 1s - loss: 8448.3750 - mean_squared_error: 8448.3750 - mean_squared_logarithmic_e\n",
      "Epoch 73/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 8523.9971 - mean_squared_error: 8523.9971 - mean_squared_logarithmic_error: 0.6766 - det_coeff: -inf - val_loss: 14025.4502 - val_mean_squared_error: 14025.4502 - val_mean_squared_logarithmic_error: 0.7976 - val_det_coeff: -inf\n",
      "Epoch 74/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 8309.5537 - mean_squared_error: 8309.5537 - mean_squared_logarithmic_error: 0.6790 - det_coeff: -inf - val_loss: 13704.9463 - val_mean_squared_error: 13704.9463 - val_mean_squared_logarithmic_error: 0.7035 - val_det_coeff: -inf\n",
      "Epoch 75/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 8456.2471 - mean_squared_error: 8456.2471 - mean_squared_logarithmic_error: 0.6821 - det_coeff: -inf - val_loss: 13622.1475 - val_mean_squared_error: 13622.1475 - val_mean_squared_logarithmic_error: 0.7869 - val_det_coeff: -infrror: 8414.9893 - mean_squared_logarithmic_error: 0.6817 - det_coeff: -i\n",
      "Epoch 76/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 8193.5303 - mean_squared_error: 8193.5303 - mean_squared_logarithmic_error: 0.6744 - det_coeff: -inf - val_loss: 13367.5811 - val_mean_squared_error: 13367.5811 - val_mean_squared_logarithmic_error: 0.8139 - val_det_coeff: -infarithmic_error: 0.6437 - det_c - ETA: 5s - loss: 7870.8389 - mean_squared_error: 7870.8389 - mean_squared_logarithmic_error: 0.6537 - det_ - ETA: 5s - loss: 7828.2744 - mean_squared_error: 7828.2744 - mean_squared_logarithmic_error - ETA: 3s - loss: 7944.2295 - mean_squared_error: 7944.2295 - mean_squared_logarithmic_error: 0.6593 - det_coeff:  - ETA: 3s - loss: 7911.8423 - mean_squared_error: 7911.8423 - mean_squared_logarithmic_error: 0.6 - ETA: 2s - loss: 8308.9590 - mean_squared_error: 8308.9590 - me\n",
      "Epoch 77/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 8302.3965 - mean_squared_error: 8302.3965 - mean_squared_logarithmic_error: 0.6770 - det_coeff: -inf - val_loss: 13480.1816 - val_mean_squared_error: 13480.1816 - val_mean_squared_logarithmic_error: 0.7555 - val_det_coeff: -inf\n",
      "Epoch 78/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 8112.9614 - mean_squared_error: 8112.9614 - mean_squared_logarithmic_error: 0.6742 - det_coeff: -inf - val_loss: 12715.9072 - val_mean_squared_error: 12715.9072 - val_mean_squared_logarithmic_error: 0.7344 - val_det_coeff: -inf 7667.3501 - mean_squared_logarithmic_e\n",
      "Epoch 79/100\n",
      "12250/12250 [==============================] - 15s 1ms/step - loss: 8111.2646 - mean_squared_error: 8111.2646 - mean_squared_logarithmic_error: 0.6693 - det_coeff: -inf - val_loss: 12848.8428 - val_mean_squared_error: 12848.8428 - val_mean_squared_logarithmic_error: 0.7628 - val_det_coeff: -infn_squared_error: 8095.2520 - mean_squared_logarithmic_error: 0.6694 - de\n",
      "Epoch 80/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 8025.8755 - mean_squared_error: 8025.8755 - mean_squared_logarithmic_error: 0.6692 - det_coeff: -inf - val_loss: 13120.1143 - val_mean_squared_error: 13120.1143 - val_mean_squared_logarithmic_error: 0.7389 - val_det_coeff: -inf\n",
      "Epoch 81/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 7819.9106 - mean_squared_error: 7819.9106 - mean_squared_logarithmic_error: 0.6669 - det_coeff: -inf - val_loss: 13228.3535 - val_mean_squared_error: 13228.3535 - val_mean_squared_logarithmic_error: 0.7515 - val_det_coeff: -infror: 7966.0132 - mean_squared_logarithm\n",
      "Epoch 82/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 7832.1284 - mean_squared_error: 7832.1284 - mean_squared_logarithmic_error: 0.6744 - det_coeff: -inf - val_loss: 13936.9873 - val_mean_squared_error: 13936.9873 - val_mean_squared_logarithmic_error: 0.7504 - val_det_coeff: -inf3 - det - ETA: 5s - \n",
      "Epoch 83/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 7914.9004 - mean_squared_error: 7914.9004 - mean_squared_logarithmic_error: 0.6705 - det_coeff: -inf - val_loss: 13037.3223 - val_mean_squared_error: 13037.3223 - val_mean_squared_logarithmic_error: 0.7300 - val_det_coeff: -inf\n",
      "Epoch 84/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 7729.0259 - mean_squared_error: 7729.0259 - mean_squared_logarithmic_error: 0.6614 - det_coeff: -inf - val_loss: 13286.1396 - val_mean_squared_error: 13286.1396 - val_mean_squared_logarithmic_error: 0.8279 - val_det_coeff: -inf637.6401 - mean_squared_logarithmic_error: 0. - ETA: 3s - loss: 7571.4121 - mean_squared_error: 7571.4121 - mean_squa - ETA: 1s - loss: 7484.7256 - mean_squared_error: 7484.7256 - mean_squared_logarithmic_error: 0.6556 - de - ETA: 0s - loss: 7550.0679 - mean_squared_error: 7550.0679 - mean_squared_logarithmic_error: 0.658\n",
      "Epoch 85/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 7657.8999 - mean_squared_error: 7657.8999 - mean_squared_logarithmic_error: 0.6689 - det_coeff: -inf - val_loss: 13601.1836 - val_mean_squared_error: 13601.1836 - val_mean_squared_logarithmic_error: 0.7858 - val_det_coeff: -inf - mean_squared_e - ETA: 5s - loss: 6667.7212 - mean_squared_error: 6667.7212 - mean_squared_logarithmi - ETA: 4s - loss: 7223.3452 - mean_s\n",
      "Epoch 86/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 7599.2510 - mean_squared_error: 7599.2510 - mean_squared_logarithmic_error: 0.6687 - det_coeff: -inf - val_loss: 12738.5186 - val_mean_squared_error: 12738.5186 - val_mean_squared_logarithmic_error: 0.7536 - val_det_coeff: -inf\n",
      "Epoch 87/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 7532.3691 - mean_squared_error: 7532.3691 - mean_squared_logarithmic_error: 0.6602 - det_coeff: -inf - val_loss: 12927.9102 - val_mean_squared_error: 12927.9102 - val_mean_squared_logarithmic_error: 0.7926 - val_det_coeff: -inf628.2637 - mean_squared_error: 7628.2637 - mean_squared_logarithmic_error: 0.6664 - de - ETA: 1s - loss: 7562.6836 - mean_squared_error: 7562.6836 - mean_squared_logarithmic_error: 0.6651 - det_coe - ETA: 1s - loss: 7484.1509 - mean_squared_error: 7484.1509 - mean_squared_logarit\n",
      "Epoch 88/100\n",
      "12250/12250 [==============================] - 15s 1ms/step - loss: 7538.5098 - mean_squared_error: 7538.5098 - mean_squared_logarithmic_error: 0.6635 - det_coeff: -inf - val_loss: 13291.0869 - val_mean_squared_error: 13291.0869 - val_mean_squared_logarithmic_error: 0.7841 - val_det_coeff: -inf 675 - ETA: 8s - loss: 7449.9375 - mean_squared_error: 7449.9375 - mean_squared_logarithmic_error: 0 - ETA: 7s - loss: 7428.9570 - mean_squared_error: 7428.9570 - mean_squar - ETA: 5s - loss: 7299.0928 - mean_squared_error: 7299.0928 - mean_squared_logarithmic_ - ETA: 3s - loss: 7372.6904 - mean_squared_error: 7372.6904 - mean_squared_logarithmic_error: 0.6568 - det_coeff: - - ETA: 3s - loss: 7395.6411 - mean_squared_error: 7395.6411 - mean_squared_logarithmic_error: 0. - ETA: 2s - loss: 7410.6504 - mean_squared_error: 7410.6504 - mean_squared_logarithmic_error: 0.658 - ETA: 1s - loss: 7514.8384 - mean_squared_error: 7514.8384 - mean_squared_logarithmic_error: 0.6600 - det_coeff: - - ETA: 1s - loss: 7487.4189 - mean_squared_error: 7487.4189 - mean_squared_logarithmic_error: 0.6609 - det_coeff: - - ETA: 1s - loss: 7466.7432 - mean_squared_error: 7466.7432 - mean_squared_logarithmic_error: 0.6\n",
      "Epoch 89/100\n",
      "12250/12250 [==============================] - 15s 1ms/step - loss: 7386.6099 - mean_squared_error: 7386.6099 - mean_squared_logarithmic_error: 0.6528 - det_coeff: -inf - val_loss: 14099.1768 - val_mean_squared_error: 14099.1768 - val_mean_squared_logarithmic_error: 0.7543 - val_det_coeff: -inf mean_squared_error: 9865.3418 - mean_squared_log - ETA: 10s - loss: 7636.5869 - mean_squared_error: 7636.5869 - mean_squared_logarithmic_error: 0. - ETA: 9s - loss: 7109.0703 - mean_squared_error: 7109.0703 - mean_squared_logarithmic_error: 0.6471 - d - ETA: 2s - loss: 7603.0396 - mean_squared_error: 7603.0396 - mean_sq\n",
      "Epoch 90/100\n",
      "12250/12250 [==============================] - 15s 1ms/step - loss: 7451.1885 - mean_squared_error: 7451.1885 - mean_squared_logarithmic_error: 0.6527 - det_coeff: -inf - val_loss: 12891.2324 - val_mean_squared_error: 12891.2324 - val_mean_squared_logarithmic_error: 0.7640 - val_det_coeff: -infor: 7163.9131 - mean_square - ETA: 7s - loss: 6968.5068 - mean_squared_error: 6968.5068 - mean_squared_logarithmic_error: 0.6519 - det_ - ETA: 7s - loss: 6913.5088 - mean_squared_error: 6913.5088 - mean_squared_logarithmic_error: 0.6568 - det_coef - ETA: 6s - loss: 6800.4395 - mean_squared_error: 6800.4395 - mean_squared_logarithmic_error: 0 - ETA: 5s - los\n",
      "Epoch 91/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 7276.0156 - mean_squared_error: 7276.0156 - mean_squared_logarithmic_error: 0.6582 - det_coeff: -inf - val_loss: 12479.0820 - val_mean_squared_error: 12479.0820 - val_mean_squared_logarithmic_error: 0.7460 - val_det_coeff: -infuared_error: 8217.4287 - - ETA: 0s - loss: 7344.6636 - mean_squared_error: 7344.6636 - mean_squared_logarithmic_error: 0.6595 - det_coeff\n",
      "Epoch 92/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 7212.7954 - mean_squared_error: 7212.7954 - mean_squared_logarithmic_error: 0.6632 - det_coeff: -inf - val_loss: 13088.0049 - val_mean_squared_error: 13088.0049 - val_mean_squared_logarithmic_error: 0.7378 - val_det_coeff: -inf\n",
      "Epoch 93/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 7141.4033 - mean_squared_error: 7141.4033 - mean_squared_logarithmic_error: 0.6600 - det_coeff: -inf - val_loss: 12724.9932 - val_mean_squared_error: 12724.9932 - val_mean_squared_logarithmic_error: 0.7149 - val_det_coeff: -inf\n",
      "Epoch 94/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 7165.9570 - mean_squared_error: 7165.9570 - mean_squared_logarithmic_error: 0.6476 - det_coeff: -inf - val_loss: 12204.2793 - val_mean_squared_error: 12204.2793 - val_mean_squared_logarithmic_error: 0.7261 - val_det_coeff: -inf mean_squared_error:  - ETA: 3s - loss: 7055.5220 - mean_squared_error: 7055.5220 - mean_squared_logarithmic_error: 0.6437 - det_co - ETA: 3s - loss: 7015.5620 - mean_squared_error: 7015.5620 - mean_squared_logarithmic_error: 0.6467 - det_coeff: - ETA: 2s - loss: 7055.4517 - mean_squared_error: 7055.4517 -\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12250/12250 [==============================] - 14s 1ms/step - loss: 7205.7935 - mean_squared_error: 7205.7935 - mean_squared_logarithmic_error: 0.6558 - det_coeff: -inf - val_loss: 12468.0908 - val_mean_squared_error: 12468.0908 - val_mean_squared_logarithmic_error: 0.7117 - val_det_coeff: -inf\n",
      "Epoch 96/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 7001.9385 - mean_squared_error: 7001.9385 - mean_squared_logarithmic_error: 0.6503 - det_coeff: -inf - val_loss: 12655.1416 - val_mean_squared_error: 12655.1416 - val_mean_squared_logarithmic_error: 0.7551 - val_det_coeff: -inf\n",
      "Epoch 97/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 6961.7334 - mean_squared_error: 6961.7334 - mean_squared_logarithmic_error: 0.6587 - det_coeff: -inf - val_loss: 12182.4082 - val_mean_squared_error: 12182.4082 - val_mean_squared_logarithmic_error: 0.6969 - val_det_coeff: -infan_squared_error: 6920.9702 - mean_squared_logarithmic_e\n",
      "Epoch 98/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 7066.4727 - mean_squared_error: 7066.4727 - mean_squared_logarithmic_error: 0.6558 - det_coeff: -inf - val_loss: 15869.4629 - val_mean_squared_error: 15869.4629 - val_mean_squared_logarithmic_error: 0.8256 - val_det_coeff: -inf - mean_squared_logarithmic_error:  - ETA: 6s - loss: 6673.8818 - mean_squared_error: 6673.8818 - mean_squared_logarithmic_err - ETA: 5s - loss: 6420.6 - ETA: 0s - loss: 7058.9131 - mean_squared_error: 7058.9131 - mean_squared_logarithmic_error: 0.6549 - d\n",
      "Epoch 99/100\n",
      "12250/12250 [==============================] - 13s 1ms/step - loss: 6982.5908 - mean_squared_error: 6982.5908 - mean_squared_logarithmic_error: 0.6585 - det_coeff: -inf - val_loss: 13207.7197 - val_mean_squared_error: 13207.7197 - val_mean_squared_logarithmic_error: 0.7557 - val_det_coeff: -inf627.9360 - mean_squared_error: 662 - ETA: 3s - loss: 6984.8628 - mean_squared_er\n",
      "Epoch 100/100\n",
      "12250/12250 [==============================] - 14s 1ms/step - loss: 6878.7114 - mean_squared_error: 6878.7114 - mean_squared_logarithmic_error: 0.6522 - det_coeff: -inf - val_loss: 13384.4180 - val_mean_squared_error: 13384.4180 - val_mean_squared_logarithmic_error: 0.7951 - val_det_coeff: -inf\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_pca, y_train, batch_size=2, epochs=100, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:45:14.642906Z",
     "start_time": "2021-05-10T21:45:14.621925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'mean_squared_error', 'mean_squared_logarithmic_error', 'det_coeff', 'val_loss', 'val_mean_squared_error', 'val_mean_squared_logarithmic_error', 'val_det_coeff'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:46:02.672717Z",
     "start_time": "2021-05-10T21:46:02.247916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d09332d640>]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T20:01:26.198794Z",
     "start_time": "2021-05-10T20:01:25.495334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: MSE: 8415.242815932415 EVS: 0.791611134779733 R2: 0.7910709893715125\n",
      "Test Score: MSE: 13037.545496267461 EVS: 0.6541490753629551 R2: 0.6540937355663705\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, explained_variance_score,r2_score, mean_squared_log_error\n",
    "\n",
    "pred_train = model.predict(X_train_pca)\n",
    "print(\"Train score:\", \"MSE:\",mean_squared_error(y_train,pred_train),\"EVS:\",explained_variance_score(y_train,pred_train),\"R2:\",r2_score(y_train,pred_train))\n",
    "\n",
    "pred = model.predict(X_test_pca)\n",
    "print(\"Test Score:\", \"MSE:\", mean_squared_error(y_test,pred),\"EVS:\",explained_variance_score(y_test,pred),\"R2:\",r2_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-10T22:45:53.989Z"
    }
   },
   "outputs": [],
   "source": [
    "# validation curve\n",
    "input_layer = Input(shape=(X_test_pca.shape[1],))\n",
    "dense_layer_1 = Dense(100, activation='relu')(input_layer)\n",
    "dense_layer_2 = Dense(50, activation='relu')(dense_layer_1)\n",
    "dense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\n",
    "output = Dense(1)(dense_layer_3)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\",\"mean_squared_logarithmic_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_val = model.fit(X_test_pca, y_train, batch_size=2, epochs=100, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:23:35.933634Z",
     "start_time": "2021-05-10T21:17:03.759534Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [0.77287256 0.75611395 0.75767496]\n",
      "CV accuracy: 0.762 +/- 0.008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator=RandomForestRegressor(max_depth=None, random_state=42),\n",
    "                         X=X_train_pca,\n",
    "                         y=y_train,\n",
    "                         cv=3,\n",
    "                         n_jobs=1)\n",
    "print('CV accuracy scores: %s' % scores)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:39:09.952954Z",
     "start_time": "2021-05-10T21:23:36.971524Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [0.81833149 0.79541575 0.77822127 0.7775064  0.7816467 ]\n",
      "CV accuracy: 0.790 +/- 0.015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator=RandomForestRegressor(max_depth=None, random_state=42),\n",
    "                         X=X_train_pca,\n",
    "                         y=y_train,\n",
    "                         cv=5,\n",
    "                         n_jobs=1)\n",
    "print('CV accuracy scores: %s' % scores)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation curve vs learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:44:14.704462Z",
     "start_time": "2021-05-10T21:39:10.452298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\lzowe\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-2e5e1d59944f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 learning_curve(estimator=RandomForestRegressor(max_depth=None, random_state=42),\n\u001b[0m\u001b[0;32m      6\u001b[0m                                \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train_pca\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times)\u001b[0m\n\u001b[0;32m   1277\u001b[0m                 \u001b[0mtrain_test_proportions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_train_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1279\u001b[1;33m         out = parallel(delayed(_fit_and_score)(\n\u001b[0m\u001b[0;32m   1280\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    387\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                                         indices=indices)\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores =\\\n",
    "                learning_curve(estimator=RandomForestRegressor(max_depth=None, random_state=42),\n",
    "                               X=X_train_pca,\n",
    "                               y=y_train,\n",
    "                               train_sizes=np.linspace(0.1, 1.0, 4),\n",
    "                               cv=None,\n",
    "                               n_jobs=1)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training examples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1.03])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/06_05.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:44:14.709448Z",
     "start_time": "2021-05-10T21:17:11.097Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "                estimator=RandomForestRegressor(max_depth=None, random_state=42), \n",
    "                X=X_train, \n",
    "                y=y_train, \n",
    "                cv=None)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='Training accuracy')\n",
    "\n",
    "plt.fill_between(param_range, train_mean + train_std,\n",
    "                 train_mean - train_std, alpha=0.15,\n",
    "                 color='blue')\n",
    "\n",
    "plt.plot(param_range, test_mean, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='Validation accuracy')\n",
    "\n",
    "plt.fill_between(param_range, \n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/06_06.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make it pipeline and apply to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "430.219px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
